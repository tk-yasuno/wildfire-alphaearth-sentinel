{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd20269",
   "metadata": {},
   "source": [
    "# ğŸ”¥ AlphaEarth Ã— Google Earth Engine ç«ç½æ¤œçŸ¥MVP\n",
    "\n",
    "æœ€æ–°ã®**AlphaEarth Foundations API**ã¨**Google Earth Engine**ã‚’æ´»ç”¨ã—ãŸ\n",
    "é«˜åº¦ãªè¡›æ˜Ÿç”»åƒç«ç½æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "\n",
    "## ğŸ¯ ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦\n",
    "- **è¡›æ˜Ÿç”»åƒAIç†è§£**: AlphaEarthã«ã‚ˆã‚‹æ„å‘³çš„åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ\n",
    "- **æ™‚ç³»åˆ—å¤‰åŒ–åˆ†æ**: ç«ç½å‰å¾Œã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æ¯”è¼ƒ\n",
    "- **ç•°å¸¸æ¤œçŸ¥**: ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã«ã‚ˆã‚‹ç«ç½ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º\n",
    "\n",
    "## ğŸ›°ï¸ å¯¾è±¡åœ°åŸŸ\n",
    "- **åœ°ç‚¹**: ã‚«ãƒªãƒ•ã‚©ãƒ«ãƒ‹ã‚¢å· æ—¢çŸ¥ç«ç½åœ°åŸŸ\n",
    "- **ãƒ‡ãƒ¼ã‚¿**: Sentinel-2 å…‰å­¦ç”»åƒ\n",
    "- **æœŸé–“**: ç«ç½å‰ãƒ»ç™ºç”Ÿä¸­ãƒ»é®ç«å¾Œ å„5æ—¥é–“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a09020a",
   "metadata": {},
   "source": [
    "## ğŸ“¦ å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce9735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"âœ… {package} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ {package} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—: {e}\")\n",
    "\n",
    "# å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ\n",
    "required_packages = [\n",
    "    \"earthengine-api\",\n",
    "    \"geemap\", \n",
    "    \"folium\",\n",
    "    \"scikit-learn\",\n",
    "    \"requests\",\n",
    "    \"numpy\",\n",
    "    \"matplotlib\",\n",
    "    \"pandas\",\n",
    "    \"seaborn\",\n",
    "    \"Pillow\"\n",
    "]\n",
    "\n",
    "print(\"ğŸš€ AlphaEarthç«ç½æ¤œçŸ¥MVPç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«é–‹å§‹...\")\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nğŸ¯ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ï¼Google Earth Engineèªè¨¼ã®æº–å‚™å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51f3eb",
   "metadata": {},
   "source": [
    "## ğŸ” Google Earth Engine èªè¨¼ã¨åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Google Earth Engineèªè¨¼\n",
    "try:\n",
    "    # æ—¢ã«èªè¨¼æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯\n",
    "    ee.Initialize()\n",
    "    print(\"âœ… Google Earth Engine æ—¢ã«èªè¨¼æ¸ˆã¿\")\n",
    "except Exception as e:\n",
    "    print(\"ğŸ” Google Earth Engine èªè¨¼ãŒå¿…è¦ã§ã™\")\n",
    "    print(\"ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:\")\n",
    "    print(\"1. ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§: earthengine authenticate\")\n",
    "    print(\"2. ãƒ–ãƒ©ã‚¦ã‚¶ã§èªè¨¼å®Œäº†å¾Œã€å†åº¦ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œ\")\n",
    "    \n",
    "    # èªè¨¼è©¦è¡Œ\n",
    "    try:\n",
    "        ee.Authenticate()\n",
    "        ee.Initialize()\n",
    "        print(\"âœ… Google Earth Engine èªè¨¼å®Œäº†\")\n",
    "    except Exception as auth_error:\n",
    "        print(f\"âŒ èªè¨¼ã‚¨ãƒ©ãƒ¼: {auth_error}\")\n",
    "        print(\"æ‰‹å‹•ã§ã®èªè¨¼ãŒå¿…è¦ã§ã™\")\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š\n",
    "plt.rcParams['font.family'] = ['Yu Gothic', 'Meiryo', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"ğŸŒ Google Earth Engine åˆæœŸåŒ–å®Œäº†\")\n",
    "print(\"ğŸ¨ å¯è¦–åŒ–ç’°å¢ƒè¨­å®šå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6adb335",
   "metadata": {},
   "source": [
    "## ğŸ”¥ ã‚«ãƒªãƒ•ã‚©ãƒ«ãƒ‹ã‚¢ç«ç½åœ°åŸŸè¨­å®š\n",
    "\n",
    "### å¯¾è±¡ç«ç½åœ°åŸŸã®é¸å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚«ãƒªãƒ•ã‚©ãƒ«ãƒ‹ã‚¢å·ã®è‘—åãªç«ç½ç™ºç”Ÿåœ°åŸŸ\n",
    "california_fire_locations = {\n",
    "    'Camp_Fire_2018': {\n",
    "        'name': 'Camp Fire 2018 (Paradise)',\n",
    "        'coordinates': [-121.6, 39.8],  # [longitude, latitude]\n",
    "        'fire_start': '2018-11-08',\n",
    "        'fire_end': '2018-11-25',\n",
    "        'description': 'ã‚«ãƒªãƒ•ã‚©ãƒ«ãƒ‹ã‚¢å²ä¸Šæœ€ã‚‚ç ´å£Šçš„ãªç«ç½',\n",
    "        'area_burned': '62,053 hectares'\n",
    "    },\n",
    "    'Thomas_Fire_2017': {\n",
    "        'name': 'Thomas Fire 2017 (Ventura County)', \n",
    "        'coordinates': [-119.3, 34.4],\n",
    "        'fire_start': '2017-12-04',\n",
    "        'fire_end': '2018-01-12', \n",
    "        'description': 'ãƒ™ãƒ³ãƒãƒ¥ãƒ©éƒ¡ã¨ã‚µãƒ³ã‚¿ãƒãƒ¼ãƒãƒ©éƒ¡ã®å¤§è¦æ¨¡ç«ç½',\n",
    "        'area_burned': '114,078 hectares'\n",
    "    },\n",
    "    'Dixie_Fire_2021': {\n",
    "        'name': 'Dixie Fire 2021 (Butte County)',\n",
    "        'coordinates': [-121.0, 40.0],\n",
    "        'fire_start': '2021-07-13',\n",
    "        'fire_end': '2021-10-25',\n",
    "        'description': 'ã‚«ãƒªãƒ•ã‚©ãƒ«ãƒ‹ã‚¢å²ä¸Š2ç•ªç›®ã«å¤§ããªç«ç½',\n",
    "        'area_burned': '390,124 hectares'\n",
    "    },\n",
    "    'Creek_Fire_2020': {\n",
    "        'name': 'Creek Fire 2020 (Fresno County)',\n",
    "        'coordinates': [-119.2, 37.2],\n",
    "        'fire_start': '2020-09-04', \n",
    "        'fire_end': '2020-12-24',\n",
    "        'description': 'ã‚·ã‚¨ãƒ©å›½æœ‰æ—ã§ã®å¤§è¦æ¨¡ç«ç½',\n",
    "        'area_burned': '154,049 hectares'\n",
    "    }\n",
    "}\n",
    "\n",
    "# ãƒ‡ãƒ¢ç”¨ã« Thomas Fire 2017 ã‚’é¸æŠï¼ˆç”»åƒãƒ‡ãƒ¼ã‚¿ã®å¯ç”¨æ€§ãŒé«˜ã„ï¼‰\n",
    "selected_fire = 'Thomas_Fire_2017'\n",
    "fire_info = california_fire_locations[selected_fire]\n",
    "\n",
    "print(f\"ğŸ”¥ é¸æŠã•ã‚ŒãŸç«ç½åœ°åŸŸ: {fire_info['name']}\")\n",
    "print(f\"ğŸ“ åº§æ¨™: {fire_info['coordinates']}\")\n",
    "print(f\"ğŸ—“ï¸ ç«ç½æœŸé–“: {fire_info['fire_start']} ï½ {fire_info['fire_end']}\")\n",
    "print(f\"ğŸ“ èª¬æ˜: {fire_info['description']}\")\n",
    "print(f\"ğŸ”¥ ç„¼å¤±é¢ç©: {fire_info['area_burned']}\")\n",
    "\n",
    "# Google Earth Engineç”¨ã®åœ°ç†çš„ç¯„å›²è¨­å®š\n",
    "longitude, latitude = fire_info['coordinates']\n",
    "roi_size = 0.05  # ç´„5kmå››æ–¹\n",
    "\n",
    "# é–¢å¿ƒé ˜åŸŸï¼ˆROIï¼‰ã‚’å®šç¾©\n",
    "roi = ee.Geometry.Rectangle([\n",
    "    longitude - roi_size,  # west\n",
    "    latitude - roi_size,   # south \n",
    "    longitude + roi_size,  # east\n",
    "    latitude + roi_size    # north\n",
    "])\n",
    "\n",
    "print(f\"\\nğŸ¯ é–¢å¿ƒé ˜åŸŸè¨­å®šå®Œäº†\")\n",
    "print(f\"   ä¸­å¿ƒåº§æ¨™: ({longitude:.3f}, {latitude:.3f})\")\n",
    "print(f\"   ç¯„å›²: ç´„{roi_size*2*111:.1f}km Ã— {roi_size*2*111:.1f}km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbad6ed",
   "metadata": {},
   "source": [
    "## ğŸ›°ï¸ Sentinel-2 ç”»åƒåé›†ã‚·ã‚¹ãƒ†ãƒ \n",
    "\n",
    "### æ™‚æœŸåˆ¥ç”»åƒãƒ‡ãƒ¼ã‚¿åé›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f426b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentinel2ImageCollector:\n",
    "    \"\"\"Sentinel-2ç”»åƒåé›†ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    \n",
    "    def __init__(self, roi, fire_start_date):\n",
    "        self.roi = roi\n",
    "        self.fire_start = datetime.strptime(fire_start_date, '%Y-%m-%d')\n",
    "        \n",
    "        # åˆ†ææœŸé–“ã®è¨­å®š\n",
    "        self.periods = {\n",
    "            'pre_fire': {\n",
    "                'start': (self.fire_start - timedelta(days=10)).strftime('%Y-%m-%d'),\n",
    "                'end': (self.fire_start - timedelta(days=1)).strftime('%Y-%m-%d'),\n",
    "                'description': 'ç«ç½ç™ºç”Ÿå‰æœŸé–“ï¼ˆ10æ—¥é–“ï¼‰'\n",
    "            },\n",
    "            'during_fire': {\n",
    "                'start': self.fire_start.strftime('%Y-%m-%d'),\n",
    "                'end': (self.fire_start + timedelta(days=9)).strftime('%Y-%m-%d'),\n",
    "                'description': 'ç«ç½ç™ºç”Ÿä¸­æœŸé–“ï¼ˆ10æ—¥é–“ï¼‰'\n",
    "            },\n",
    "            'post_fire': {\n",
    "                'start': (self.fire_start + timedelta(days=10)).strftime('%Y-%m-%d'),\n",
    "                'end': (self.fire_start + timedelta(days=19)).strftime('%Y-%m-%d'),\n",
    "                'description': 'ç«ç½ç™ºç”Ÿå¾ŒæœŸé–“ï¼ˆ10æ—¥é–“ï¼‰'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def collect_images(self, max_cloud_cover=20):\n",
    "        \"\"\"å„æœŸé–“ã®Sentinel-2ç”»åƒã‚’åé›†\"\"\"\n",
    "        \n",
    "        collected_images = {}\n",
    "        \n",
    "        print(\"ğŸ›°ï¸ Sentinel-2ç”»åƒåé›†é–‹å§‹...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for period_name, period_info in self.periods.items():\n",
    "            print(f\"\\nğŸ“… {period_info['description']}\")\n",
    "            print(f\"   æœŸé–“: {period_info['start']} ï½ {period_info['end']}\")\n",
    "            \n",
    "            try:\n",
    "                # Sentinel-2ç”»åƒã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å–å¾—\n",
    "                collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "                    .filterBounds(self.roi) \\\n",
    "                    .filterDate(period_info['start'], period_info['end']) \\\n",
    "                    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', max_cloud_cover)) \\\n",
    "                    .select(['B2', 'B3', 'B4', 'B8', 'B11', 'B12'])  # Blue, Green, Red, NIR, SWIR1, SWIR2\n",
    "                \n",
    "                # ç”»åƒæ•°ç¢ºèª\n",
    "                image_count = collection.size().getInfo()\n",
    "                print(f\"   ğŸ“Š å–å¾—ç”»åƒæ•°: {image_count}æš\")\n",
    "                \n",
    "                if image_count > 0:\n",
    "                    # ä¸­å¤®å€¤åˆæˆï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰ã®å½±éŸ¿ã‚’è»½æ¸›ï¼‰\n",
    "                    composite_image = collection.median().clip(self.roi)\n",
    "                    \n",
    "                    # ç”»åƒã®åŸºæœ¬çµ±è¨ˆæƒ…å ±ã‚’å–å¾—\n",
    "                    image_stats = self._get_image_statistics(composite_image)\n",
    "                    \n",
    "                    collected_images[period_name] = {\n",
    "                        'image': composite_image,\n",
    "                        'collection': collection,\n",
    "                        'period_info': period_info,\n",
    "                        'image_count': image_count,\n",
    "                        'statistics': image_stats,\n",
    "                        'status': 'success'\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"   âœ… åˆæˆç”»åƒä½œæˆå®Œäº†\")\n",
    "                    print(f\"   ğŸ“ˆ å¹³å‡åå°„ç‡ (Red): {image_stats.get('B4_mean', 'N/A'):.3f}\")\n",
    "                    \n",
    "                else:\n",
    "                    collected_images[period_name] = {\n",
    "                        'status': 'no_data',\n",
    "                        'period_info': period_info,\n",
    "                        'image_count': 0\n",
    "                    }\n",
    "                    print(f\"   âš ï¸ è©²å½“æœŸé–“ã«åˆ©ç”¨å¯èƒ½ãªç”»åƒãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "                collected_images[period_name] = {\n",
    "                    'status': 'error',\n",
    "                    'error': str(e),\n",
    "                    'period_info': period_info\n",
    "                }\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ç”»åƒåé›†å®Œäº†: {len([k for k, v in collected_images.items() if v.get('status') == 'success'])}æœŸé–“æˆåŠŸ\")\n",
    "        return collected_images\n",
    "    \n",
    "    def _get_image_statistics(self, image):\n",
    "        \"\"\"ç”»åƒã®åŸºæœ¬çµ±è¨ˆæƒ…å ±ã‚’å–å¾—\"\"\"\n",
    "        try:\n",
    "            # å„ãƒãƒ³ãƒ‰ã®å¹³å‡å€¤ã‚’è¨ˆç®—\n",
    "            stats = image.reduceRegion(\n",
    "                reducer=ee.Reducer.mean(),\n",
    "                geometry=self.roi,\n",
    "                scale=20,  # Sentinel-2ã®è§£åƒåº¦\n",
    "                maxPixels=1e9\n",
    "            ).getInfo()\n",
    "            \n",
    "            return stats\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ çµ±è¨ˆè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def create_visualization_map(self, collected_images):\n",
    "        \"\"\"åé›†ã—ãŸç”»åƒã®å¯è¦–åŒ–ãƒãƒƒãƒ—ã‚’ä½œæˆ\"\"\"\n",
    "        \n",
    "        # åœ°å›³ã®åˆæœŸåŒ–\n",
    "        center_lat = self.roi.centroid().coordinates().getInfo()[1]\n",
    "        center_lon = self.roi.centroid().coordinates().getInfo()[0]\n",
    "        \n",
    "        m = geemap.Map(center=[center_lat, center_lon], zoom=12)\n",
    "        \n",
    "        # ROIã‚’åœ°å›³ã«è¿½åŠ \n",
    "        roi_style = {'color': 'red', 'fillOpacity': 0.1}\n",
    "        m.addLayer(self.roi, {}, 'ROI', opacity=0.5)\n",
    "        \n",
    "        # å„æœŸé–“ã®ç”»åƒã‚’åœ°å›³ã«è¿½åŠ \n",
    "        vis_params = {\n",
    "            'bands': ['B4', 'B3', 'B2'],  # RGB\n",
    "            'min': 0,\n",
    "            'max': 3000,\n",
    "            'gamma': 1.4\n",
    "        }\n",
    "        \n",
    "        colors = ['blue', 'red', 'green']\n",
    "        period_names = ['pre_fire', 'during_fire', 'post_fire']\n",
    "        \n",
    "        for i, period_name in enumerate(period_names):\n",
    "            if period_name in collected_images and collected_images[period_name].get('status') == 'success':\n",
    "                image = collected_images[period_name]['image']\n",
    "                period_desc = collected_images[period_name]['period_info']['description']\n",
    "                \n",
    "                m.addLayer(image, vis_params, period_desc, shown=(i==0))\n",
    "        \n",
    "        return m\n",
    "\n",
    "# ç”»åƒåé›†ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n",
    "collector = Sentinel2ImageCollector(roi, fire_info['fire_start'])\n",
    "\n",
    "print(f\"ğŸ¯ ç”»åƒåé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†\")\n",
    "print(f\"ğŸ“… åˆ†ææœŸé–“è¨­å®š:\")\n",
    "for period_name, period_info in collector.periods.items():\n",
    "    print(f\"   {period_info['description']}: {period_info['start']} ï½ {period_info['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76688f4",
   "metadata": {},
   "source": [
    "## ğŸš€ ç”»åƒãƒ‡ãƒ¼ã‚¿åé›†å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f64ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2ç”»åƒã®åé›†å®Ÿè¡Œ\n",
    "print(\"ğŸ›°ï¸ Thomas Fire 2017åœ°åŸŸã®Sentinel-2ç”»åƒåé›†ã‚’é–‹å§‹...\")\n",
    "print(f\"ğŸ¯ å¯¾è±¡åœ°åŸŸ: {fire_info['name']}\")\n",
    "print(f\"ğŸ“ åº§æ¨™: {fire_info['coordinates']}\")\n",
    "\n",
    "# ç”»åƒåé›†å®Ÿè¡Œ\n",
    "satellite_images = collector.collect_images(max_cloud_cover=30)  # é›²é‡30%ä»¥ä¸‹ã®ç”»åƒã‚’åé›†\n",
    "\n",
    "# åé›†çµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º\n",
    "print(\"\\nğŸ“Š ç”»åƒåé›†çµæœã‚µãƒãƒªãƒ¼:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "success_count = 0\n",
    "total_images = 0\n",
    "\n",
    "for period_name, data in satellite_images.items():\n",
    "    status = data.get('status', 'unknown')\n",
    "    period_desc = data['period_info']['description']\n",
    "    image_count = data.get('image_count', 0)\n",
    "    \n",
    "    print(f\"\\nğŸ“… {period_desc}\")\n",
    "    print(f\"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}\")\n",
    "    print(f\"   ç”»åƒæ•°: {image_count}æš\")\n",
    "    \n",
    "    if status == 'success':\n",
    "        success_count += 1\n",
    "        total_images += image_count\n",
    "        \n",
    "        # çµ±è¨ˆæƒ…å ±è¡¨ç¤º\n",
    "        stats = data.get('statistics', {})\n",
    "        if stats:\n",
    "            red_mean = stats.get('B4', 0)\n",
    "            nir_mean = stats.get('B8', 0)\n",
    "            print(f\"   å¹³å‡Redåå°„ç‡: {red_mean:.3f}\")\n",
    "            print(f\"   å¹³å‡NIRåå°„ç‡: {nir_mean:.3f}\")\n",
    "            \n",
    "            # NDVIè¨ˆç®—ï¼ˆæ¤ç”ŸæŒ‡æ•°ï¼‰\n",
    "            if red_mean > 0 and nir_mean > 0:\n",
    "                ndvi = (nir_mean - red_mean) / (nir_mean + red_mean)\n",
    "                print(f\"   NDVIæ¨å®šå€¤: {ndvi:.3f}\")\n",
    "    \n",
    "    elif status == 'no_data':\n",
    "        print(f\"   âš ï¸ åˆ©ç”¨å¯èƒ½ãªç”»åƒãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "    elif status == 'error':\n",
    "        error_msg = data.get('error', 'Unknown error')\n",
    "        print(f\"   âŒ ã‚¨ãƒ©ãƒ¼: {error_msg}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ åé›†å®Œäº†ã‚µãƒãƒªãƒ¼:\")\n",
    "print(f\"   æˆåŠŸæœŸé–“: {success_count}/3æœŸé–“\")\n",
    "print(f\"   ç·ç”»åƒæ•°: {total_images}æš\")\n",
    "\n",
    "if success_count >= 2:\n",
    "    print(f\"   âœ… åˆ†æã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒåé›†ã•ã‚Œã¾ã—ãŸ\")\n",
    "    analysis_ready = True\n",
    "else:\n",
    "    print(f\"   âš ï¸ åˆ†æã«ã¯æœ€ä½2æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™\")\n",
    "    analysis_ready = False\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜\n",
    "globals()['satellite_images'] = satellite_images\n",
    "globals()['analysis_ready'] = analysis_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf56cd",
   "metadata": {},
   "source": [
    "## ğŸ—ºï¸ åé›†ç”»åƒã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef810170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åé›†ã—ãŸç”»åƒã®å¯è¦–åŒ–ãƒãƒƒãƒ—ä½œæˆ\n",
    "if analysis_ready:\n",
    "    print(\"ğŸ—ºï¸ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒ—ä½œæˆä¸­...\")\n",
    "    \n",
    "    # å¯è¦–åŒ–ãƒãƒƒãƒ—ã®ä½œæˆ\n",
    "    visualization_map = collector.create_visualization_map(satellite_images)\n",
    "    \n",
    "    print(\"âœ… å¯è¦–åŒ–ãƒãƒƒãƒ—ä½œæˆå®Œäº†\")\n",
    "    print(\"ğŸ“ åœ°å›³ä¸Šã§å„æœŸé–“ã®ç”»åƒãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åˆ‡ã‚Šæ›¿ãˆã¦æ¯”è¼ƒã§ãã¾ã™\")\n",
    "    print(\"ğŸ”¥ ç«ç½å‰ãƒ»ç«ç½ä¸­ãƒ»ç«ç½å¾Œã®å¤‰åŒ–ã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "    \n",
    "    # ãƒãƒƒãƒ—ã‚’è¡¨ç¤º\n",
    "    display(visualization_map)\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€å¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")\n",
    "    print(\"ğŸ”§ ä¸Šã®ã‚»ãƒ«ã§ç”»åƒåé›†ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b4e6b1",
   "metadata": {},
   "source": [
    "## ğŸ“Š åé›†ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åé›†ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æã¨ã‚µãƒãƒªãƒ¼å¯è¦–åŒ–\n",
    "if analysis_ready:\n",
    "    print(\"ğŸ“Š åé›†ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æå®Ÿè¡Œä¸­...\")\n",
    "    \n",
    "    # åˆ†æçµæœç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ\n",
    "    analysis_data = []\n",
    "    \n",
    "    for period_name, data in satellite_images.items():\n",
    "        if data.get('status') == 'success':\n",
    "            stats = data.get('statistics', {})\n",
    "            period_info = data['period_info']\n",
    "            \n",
    "            # ãƒãƒ³ãƒ‰åˆ¥çµ±è¨ˆãƒ‡ãƒ¼ã‚¿\n",
    "            for band, value in stats.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    analysis_data.append({\n",
    "                        'Period': period_info['description'],\n",
    "                        'Period_Code': period_name,\n",
    "                        'Band': band,\n",
    "                        'Reflectance': value,\n",
    "                        'Date_Range': f\"{period_info['start']} to {period_info['end']}\",\n",
    "                        'Image_Count': data['image_count']\n",
    "                    })\n",
    "    \n",
    "    if analysis_data:\n",
    "        df = pd.DataFrame(analysis_data)\n",
    "        \n",
    "        # å¯è¦–åŒ–ç”¨ã®å›³ä½œæˆ\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('ğŸ›°ï¸ Sentinel-2ç”»åƒåé›†ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœ', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. æœŸé–“åˆ¥ãƒãƒ³ãƒ‰åå°„ç‡æ¯”è¼ƒ\n",
    "        ax1 = axes[0, 0]\n",
    "        main_bands = ['B2', 'B3', 'B4', 'B8']  # Blue, Green, Red, NIR\n",
    "        df_main = df[df['Band'].isin(main_bands)]\n",
    "        \n",
    "        if not df_main.empty:\n",
    "            pivot_data = df_main.pivot(index='Period_Code', columns='Band', values='Reflectance')\n",
    "            pivot_data.plot(kind='bar', ax=ax1, alpha=0.8)\n",
    "            ax1.set_title('æœŸé–“åˆ¥ä¸»è¦ãƒãƒ³ãƒ‰åå°„ç‡', fontweight='bold')\n",
    "            ax1.set_xlabel('åˆ†ææœŸé–“')\n",
    "            ax1.set_ylabel('åå°„ç‡')\n",
    "            ax1.legend(title='ãƒãƒ³ãƒ‰')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. NDVIè¨ˆç®—ã¨è¡¨ç¤º\n",
    "        ax2 = axes[0, 1]\n",
    "        ndvi_data = []\n",
    "        \n",
    "        for period_name, data in satellite_images.items():\n",
    "            if data.get('status') == 'success':\n",
    "                stats = data.get('statistics', {})\n",
    "                red = stats.get('B4', 0)\n",
    "                nir = stats.get('B8', 0)\n",
    "                \n",
    "                if red > 0 and nir > 0:\n",
    "                    ndvi = (nir - red) / (nir + red)\n",
    "                    ndvi_data.append({\n",
    "                        'Period': data['period_info']['description'],\n",
    "                        'NDVI': ndvi,\n",
    "                        'Period_Code': period_name\n",
    "                    })\n",
    "        \n",
    "        if ndvi_data:\n",
    "            ndvi_df = pd.DataFrame(ndvi_data)\n",
    "            colors = ['blue', 'red', 'green']\n",
    "            bars = ax2.bar(ndvi_df['Period_Code'], ndvi_df['NDVI'], color=colors[:len(ndvi_df)], alpha=0.7)\n",
    "            ax2.set_title('æœŸé–“åˆ¥NDVIï¼ˆæ¤ç”ŸæŒ‡æ•°ï¼‰', fontweight='bold')\n",
    "            ax2.set_xlabel('åˆ†ææœŸé–“')\n",
    "            ax2.set_ylabel('NDVIå€¤')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # NDVIå€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "            for bar, ndvi in zip(bars, ndvi_df['NDVI']):\n",
    "                height = bar.get_height()\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{ndvi:.3f}',\n",
    "                        ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 3. åé›†ç”»åƒæ•°çµ±è¨ˆ\n",
    "        ax3 = axes[1, 0]\n",
    "        image_counts = [data.get('image_count', 0) for data in satellite_images.values() \n",
    "                       if data.get('status') == 'success']\n",
    "        period_labels = [data['period_info']['description'].replace('ï¼ˆ10æ—¥é–“ï¼‰', '') \n",
    "                        for data in satellite_images.values() \n",
    "                        if data.get('status') == 'success']\n",
    "        \n",
    "        if image_counts:\n",
    "            bars = ax3.bar(period_labels, image_counts, color=['skyblue', 'orange', 'lightgreen'][:len(image_counts)], alpha=0.8)\n",
    "            ax3.set_title('æœŸé–“åˆ¥åé›†ç”»åƒæ•°', fontweight='bold')\n",
    "            ax3.set_xlabel('åˆ†ææœŸé–“')\n",
    "            ax3.set_ylabel('ç”»åƒæ•°ï¼ˆæšï¼‰')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            \n",
    "            # ç”»åƒæ•°ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "            for bar, count in zip(bars, image_counts):\n",
    "                height = bar.get_height()\n",
    "                ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1, f'{count}æš',\n",
    "                        ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 4. ãƒ‡ãƒ¼ã‚¿åé›†ã‚µãƒãƒªãƒ¼\n",
    "        ax4 = axes[1, 1]\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        # ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ\n",
    "        summary_text = f\"\"\"\n",
    "ğŸ¯ ãƒ‡ãƒ¼ã‚¿åé›†ã‚µãƒãƒªãƒ¼\n",
    "\n",
    "ğŸ“ å¯¾è±¡åœ°åŸŸ: {fire_info['name']}\n",
    "ğŸ—“ï¸ ç«ç½æœŸé–“: {fire_info['fire_start']} ï½ {fire_info['fire_end']}\n",
    "ğŸ“Š æˆåŠŸæœŸé–“: {success_count}/3æœŸé–“\n",
    "ğŸ›°ï¸ ç·ç”»åƒæ•°: {total_images}æš\n",
    "\n",
    "ğŸ“ˆ åˆ†ææº–å‚™çŠ¶æ³:\n",
    "{'âœ… AlphaEarthåˆ†ææº–å‚™å®Œäº†' if analysis_ready else 'âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚è¿½åŠ åé›†å¿…è¦'}\n",
    "\n",
    "ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n",
    "â€¢ AlphaEarthåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ\n",
    "â€¢ æ™‚ç³»åˆ—å¤‰åŒ–åˆ†æ\n",
    "â€¢ ç«ç½ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º\n",
    "        \"\"\"\n",
    "        \n",
    "        ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, fontsize=11,\n",
    "                verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"âœ… ãƒ‡ãƒ¼ã‚¿åˆ†æå¯è¦–åŒ–å®Œäº†\")\n",
    "        print(f\"ğŸ“Š {len(df)}ä»¶ã®ãƒãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ\")\n",
    "        print(f\"ğŸ¯ AlphaEarthåˆ†æã¸ã®æº–å‚™å®Œäº†\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ åˆ†æå¯èƒ½ãªçµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "else:\n",
    "    print(\"âš ï¸ åˆ†æã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™\")\n",
    "    print(\"ğŸ”§ ç”»åƒåé›†ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a377a",
   "metadata": {},
   "source": [
    "## ğŸ§  AlphaEarthåŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ \n",
    "\n",
    "### AlphaEarth Foundations APIçµ±åˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class AlphaEarthAPIClient:\n",
    "    \"\"\"AlphaEarth Foundations API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        AlphaEarth APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–\n",
    "        \n",
    "        Args:\n",
    "            api_key: AlphaEarth API ã‚­ãƒ¼ï¼ˆç’°å¢ƒå¤‰æ•° ALPHAEARTH_API_KEY ã‹ã‚‰ã‚‚å–å¾—å¯èƒ½ï¼‰\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or self._get_api_key()\n",
    "        self.base_url = \"https://api.alphaearth.ai/v1\"  # ä»®æƒ³ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        if self.api_key:\n",
    "            self.session.headers.update({\n",
    "                'Authorization': f'Bearer {self.api_key}',\n",
    "                'Content-Type': 'application/json'\n",
    "            })\n",
    "            print(\"âœ… AlphaEarth API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†\")\n",
    "        else:\n",
    "            print(\"âš ï¸ AlphaEarth API ã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "            print(\"ğŸ’¡ ãƒ‡ãƒ¢ç”¨ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ã¾ã™\")\n",
    "    \n",
    "    def _get_api_key(self) -> Optional[str]:\n",
    "        \"\"\"ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—\"\"\"\n",
    "        import os\n",
    "        \n",
    "        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ\n",
    "        api_key = os.getenv('ALPHAEARTH_API_KEY')\n",
    "        if api_key:\n",
    "            return api_key\n",
    "        \n",
    "        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ\n",
    "        try:\n",
    "            with open('.alphaearth_config', 'r') as f:\n",
    "                config = json.load(f)\n",
    "                return config.get('api_key')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def encode_image_to_embedding(self, ee_image, roi, period_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Google Earth Engineç”»åƒã‚’AlphaEarthåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›\n",
    "        \n",
    "        Args:\n",
    "            ee_image: Google Earth Engineç”»åƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "            roi: é–¢å¿ƒé ˜åŸŸ\n",
    "            period_name: æœŸé–“åï¼ˆpre_fire, during_fire, post_fireï¼‰\n",
    "            \n",
    "        Returns:\n",
    "            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨é–¢é€£ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"ğŸ§  {period_name} æœŸé–“ã®ç”»åƒåŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­...\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Earth Engineç”»åƒã‚’ãƒ­ãƒ¼ã‚«ãƒ«é…åˆ—ã«å¤‰æ›\n",
    "            image_array = self._ee_image_to_array(ee_image, roi)\n",
    "            \n",
    "            if image_array is None:\n",
    "                return {\n",
    "                    'status': 'error',\n",
    "                    'error': 'Failed to convert EE image to array',\n",
    "                    'period': period_name\n",
    "                }\n",
    "            \n",
    "            # 2. AlphaEarth APIã§åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ\n",
    "            if self.api_key:\n",
    "                # å®Ÿéš›ã®APIå‘¼ã³å‡ºã—\n",
    "                embedding = self._call_alphaearth_api(image_array, period_name)\n",
    "            else:\n",
    "                # ãƒ‡ãƒ¢ç”¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿\n",
    "                embedding = self._generate_simulation_embedding(image_array, period_name)\n",
    "            \n",
    "            # 3. åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ­£è¦åŒ–\n",
    "            normalized_embedding = normalize([embedding])[0]\n",
    "            \n",
    "            # 4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ\n",
    "            metadata = {\n",
    "                'status': 'success',\n",
    "                'period': period_name,\n",
    "                'embedding_vector': normalized_embedding.tolist(),\n",
    "                'embedding_dimension': len(normalized_embedding),\n",
    "                'image_shape': image_array.shape,\n",
    "                'generation_timestamp': datetime.now().isoformat(),\n",
    "                'processing_method': 'alphaearth_api' if self.api_key else 'simulation'\n",
    "            }\n",
    "            \n",
    "            print(f\"   âœ… åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº† (æ¬¡å…ƒ: {len(normalized_embedding)})\")\n",
    "            return metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'error': str(e),\n",
    "                'period': period_name\n",
    "            }\n",
    "    \n",
    "    def _ee_image_to_array(self, ee_image, roi, scale: int = 20) -> Optional[np.ndarray]:\n",
    "        \"\"\"Earth Engineç”»åƒã‚’NumPyé…åˆ—ã«å¤‰æ›\"\"\"\n",
    "        try:\n",
    "            # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«å–å¾—\n",
    "            image_data = ee_image.sampleRectangle(\n",
    "                region=roi,\n",
    "                defaultValue=0\n",
    "            ).getInfo()\n",
    "            \n",
    "            # ãƒãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’é…åˆ—ã«å¤‰æ›\n",
    "            bands = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']  # Blue, Green, Red, NIR, SWIR1, SWIR2\n",
    "            arrays = []\n",
    "            \n",
    "            for band in bands:\n",
    "                if band in image_data['properties']:\n",
    "                    band_array = np.array(image_data['properties'][band])\n",
    "                    arrays.append(band_array)\n",
    "            \n",
    "            if arrays:\n",
    "                # ãƒãƒ³ãƒ‰ã‚’çµåˆã—ã¦3æ¬¡å…ƒé…åˆ—ã‚’ä½œæˆ (height, width, bands)\n",
    "                combined_array = np.stack(arrays, axis=-1)\n",
    "                return combined_array\n",
    "            else:\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ ç”»åƒé…åˆ—å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _call_alphaearth_api(self, image_array: np.ndarray, period_name: str) -> np.ndarray:\n",
    "        \"\"\"å®Ÿéš›ã®AlphaEarth APIã‚’å‘¼ã³å‡ºã—ã¦åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ\"\"\"\n",
    "        \n",
    "        # ç”»åƒé…åˆ—ã‚’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "        image_bytes = io.BytesIO()\n",
    "        # ç”»åƒã‚’é©åˆ‡ãªå½¢å¼ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®APIã®ä»•æ§˜ã«å¿œã˜ã¦èª¿æ•´ï¼‰\n",
    "        np.save(image_bytes, image_array)\n",
    "        image_base64 = base64.b64encode(image_bytes.getvalue()).decode('utf-8')\n",
    "        \n",
    "        # API ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰\n",
    "        payload = {\n",
    "            'image_data': image_base64,\n",
    "            'encoding_type': 'numpy_array',\n",
    "            'model_version': 'alphaearth-foundations-v1',\n",
    "            'metadata': {\n",
    "                'period': period_name,\n",
    "                'source': 'sentinel2',\n",
    "                'application': 'wildfire_detection'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(\n",
    "                f\"{self.base_url}/embeddings/encode\",\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                embedding = np.array(result['embedding'])\n",
    "                return embedding\n",
    "            else:\n",
    "                raise Exception(f\"API Error: {response.status_code} - {response.text}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨\n",
    "            return self._generate_simulation_embedding(image_array, period_name)\n",
    "    \n",
    "    def _generate_simulation_embedding(self, image_array: np.ndarray, period_name: str) -> np.ndarray:\n",
    "        \"\"\"ãƒ‡ãƒ¢ç”¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ\"\"\"\n",
    "        \n",
    "        # ç”»åƒã®çµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’è¨ˆç®—\n",
    "        features = []\n",
    "        \n",
    "        # 1. ãƒãƒ³ãƒ‰åˆ¥å¹³å‡å€¤\n",
    "        band_means = np.mean(image_array, axis=(0, 1))\n",
    "        features.extend(band_means)\n",
    "        \n",
    "        # 2. ãƒãƒ³ãƒ‰åˆ¥æ¨™æº–åå·®\n",
    "        band_stds = np.std(image_array, axis=(0, 1))\n",
    "        features.extend(band_stds)\n",
    "        \n",
    "        # 3. NDVI (æ­£è¦åŒ–æ¤ç”ŸæŒ‡æ•°)\n",
    "        if image_array.shape[-1] >= 4:  # NIRãƒãƒ³ãƒ‰ãŒåˆ©ç”¨å¯èƒ½\n",
    "            red = image_array[:, :, 2]   # B4 (Red)\n",
    "            nir = image_array[:, :, 3]   # B8 (NIR)\n",
    "            ndvi = np.mean((nir - red) / (nir + red + 1e-8))\n",
    "            features.append(ndvi)\n",
    "        \n",
    "        # 4. ç«ç½æŒ‡æ•° (SWIR1ã¨NIRã®æ¯”ç‡)\n",
    "        if image_array.shape[-1] >= 6:\n",
    "            swir1 = image_array[:, :, 4]  # B11 (SWIR1)\n",
    "            nir = image_array[:, :, 3]    # B8 (NIR)\n",
    "            fire_index = np.mean(swir1 / (nir + 1e-8))\n",
    "            features.append(fire_index)\n",
    "        \n",
    "        # 5. ãƒ†ã‚¯ã‚¹ãƒãƒ£ç‰¹å¾´é‡ï¼ˆåˆ†æ•£ãƒ™ãƒ¼ã‚¹ï¼‰\n",
    "        texture_features = []\n",
    "        for band_idx in range(image_array.shape[-1]):\n",
    "            band = image_array[:, :, band_idx]\n",
    "            # ãƒ­ãƒ¼ã‚«ãƒ«åˆ†æ•£ã‚’è¨ˆç®—\n",
    "            from scipy import ndimage\n",
    "            local_variance = ndimage.generic_filter(band, np.var, size=3)\n",
    "            texture_features.append(np.mean(local_variance))\n",
    "        features.extend(texture_features)\n",
    "        \n",
    "        # 6. æœŸé–“ç‰¹æœ‰ã®ç‰¹å¾´é‡ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰\n",
    "        period_features = self._get_period_specific_features(period_name, len(features))\n",
    "        features.extend(period_features)\n",
    "        \n",
    "        # 512æ¬¡å…ƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ\n",
    "        target_dim = 512\n",
    "        embedding = np.array(features)\n",
    "        \n",
    "        # æ¬¡å…ƒèª¿æ•´\n",
    "        if len(embedding) < target_dim:\n",
    "            # ä¸è¶³åˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã§è£œå®Œ\n",
    "            np.random.seed(hash(period_name) % 2**32)  # å†ç¾æ€§ã®ãŸã‚\n",
    "            padding = np.random.normal(0, 0.1, target_dim - len(embedding))\n",
    "            embedding = np.concatenate([embedding, padding])\n",
    "        elif len(embedding) > target_dim:\n",
    "            # æ¬¡å…ƒå‰Šæ¸›\n",
    "            embedding = embedding[:target_dim]\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def _get_period_specific_features(self, period_name: str, current_length: int) -> List[float]:\n",
    "        \"\"\"æœŸé–“ç‰¹æœ‰ã®ç‰¹å¾´é‡ã‚’ç”Ÿæˆï¼ˆç«ç½ã®é€²è¡Œæ®µéšã‚’æ¨¡æ“¬ï¼‰\"\"\"\n",
    "        \n",
    "        # æœŸé–“åˆ¥ã®ç‰¹å¾´ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "        period_patterns = {\n",
    "            'pre_fire': {\n",
    "                'vegetation_health': 0.8,   # å¥å…¨ãªæ¤ç”Ÿ\n",
    "                'thermal_anomaly': 0.1,     # ä½ã„ç†±ç•°å¸¸\n",
    "                'smoke_indicator': 0.0,     # ç…™ãªã—\n",
    "                'spectral_change': 0.1      # ä½ã„ã‚¹ãƒšã‚¯ãƒˆãƒ«å¤‰åŒ–\n",
    "            },\n",
    "            'during_fire': {\n",
    "                'vegetation_health': 0.2,   # æ¤ç”Ÿã®æå‚·\n",
    "                'thermal_anomaly': 0.9,     # é«˜ã„ç†±ç•°å¸¸\n",
    "                'smoke_indicator': 0.8,     # ç…™ã®å­˜åœ¨\n",
    "                'spectral_change': 0.9      # é«˜ã„ã‚¹ãƒšã‚¯ãƒˆãƒ«å¤‰åŒ–\n",
    "            },\n",
    "            'post_fire': {\n",
    "                'vegetation_health': 0.1,   # æ¤ç”Ÿã®å¤§å¹…ãªæå¤±\n",
    "                'thermal_anomaly': 0.3,     # æ®‹ã‚Šç«ã«ã‚ˆã‚‹ä¸­ç¨‹åº¦ã®ç†±ç•°å¸¸\n",
    "                'smoke_indicator': 0.2,     # å°‘é‡ã®ç…™\n",
    "                'spectral_change': 0.7      # é«˜ã„ã‚¹ãƒšã‚¯ãƒˆãƒ«å¤‰åŒ–ï¼ˆç„¼è·¡ï¼‰\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        pattern = period_patterns.get(period_name, period_patterns['pre_fire'])\n",
    "        \n",
    "        # ãƒã‚¤ã‚ºã‚’è¿½åŠ ã—ã¦ç¾å®Ÿçš„ãªå¤‰å‹•ã‚’æ¨¡æ“¬\n",
    "        np.random.seed(hash(period_name + str(current_length)) % 2**32)\n",
    "        noise_scale = 0.1\n",
    "        \n",
    "        features = []\n",
    "        for key, base_value in pattern.items():\n",
    "            noisy_value = base_value + np.random.normal(0, noise_scale)\n",
    "            features.append(np.clip(noisy_value, 0, 1))  # 0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—\n",
    "        \n",
    "        return features\n",
    "\n",
    "# AlphaEarth APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–\n",
    "print(\"ğŸ§  AlphaEarthåŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–...\")\n",
    "alphaearth_client = AlphaEarthAPIClient()\n",
    "\n",
    "print(\"âœ… AlphaEarthåŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†\")\n",
    "print(\"ğŸ’¡ ç”»åƒã‹ã‚‰æ„å‘³çš„åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã§ãã¾ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b77a6d",
   "metadata": {},
   "source": [
    "## ğŸš€ è¡›æ˜Ÿç”»åƒåŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Ÿè¡Œ\n",
    "\n",
    "### å„æœŸé–“ã®ç”»åƒã‚’AlphaEarthåŸ‹ã‚è¾¼ã¿ã«å¤‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46433dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åé›†ã—ãŸè¡›æ˜Ÿç”»åƒã‹ã‚‰AlphaEarthåŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ\n",
    "if analysis_ready and 'satellite_images' in globals():\n",
    "    print(\"ğŸ§  AlphaEarthåŸ‹ã‚è¾¼ã¿ç”Ÿæˆé–‹å§‹...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # åŸ‹ã‚è¾¼ã¿çµæœã‚’æ ¼ç´\n",
    "    embeddings_data = {}\n",
    "    successful_embeddings = 0\n",
    "    \n",
    "    # å„æœŸé–“ã®ç”»åƒã«å¯¾ã—ã¦åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ\n",
    "    for period_name, image_data in satellite_images.items():\n",
    "        if image_data.get('status') == 'success':\n",
    "            print(f\"\\nğŸ›°ï¸ {image_data['period_info']['description']}\")\n",
    "            \n",
    "            # Earth Engineç”»åƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—\n",
    "            ee_image = image_data['image']\n",
    "            \n",
    "            # AlphaEarthåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ\n",
    "            embedding_result = alphaearth_client.encode_image_to_embedding(\n",
    "                ee_image, roi, period_name\n",
    "            )\n",
    "            \n",
    "            # çµæœã‚’ä¿å­˜\n",
    "            embeddings_data[period_name] = embedding_result\n",
    "            \n",
    "            if embedding_result['status'] == 'success':\n",
    "                successful_embeddings += 1\n",
    "                \n",
    "                # åŸ‹ã‚è¾¼ã¿æƒ…å ±ã‚’è¡¨ç¤º\n",
    "                embedding_dim = embedding_result['embedding_dimension']\n",
    "                processing_method = embedding_result['processing_method']\n",
    "                \n",
    "                print(f\"   âœ… åŸ‹ã‚è¾¼ã¿ç”ŸæˆæˆåŠŸ\")\n",
    "                print(f\"   ğŸ“Š æ¬¡å…ƒæ•°: {embedding_dim}\")\n",
    "                print(f\"   ğŸ”§ å‡¦ç†æ–¹æ³•: {processing_method}\")\n",
    "                \n",
    "                # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®åŸºæœ¬çµ±è¨ˆ\n",
    "                embedding_vector = np.array(embedding_result['embedding_vector'])\n",
    "                print(f\"   ğŸ“ˆ å¹³å‡å€¤: {np.mean(embedding_vector):.4f}\")\n",
    "                print(f\"   ğŸ“ˆ æ¨™æº–åå·®: {np.std(embedding_vector):.4f}\")\n",
    "                print(f\"   ğŸ“ˆ L2ãƒãƒ«ãƒ : {np.linalg.norm(embedding_vector):.4f}\")\n",
    "            else:\n",
    "                error_msg = embedding_result.get('error', 'Unknown error')\n",
    "                print(f\"   âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå¤±æ•—: {error_msg}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†ã‚µãƒãƒªãƒ¼:\")\n",
    "    print(f\"   æˆåŠŸ: {successful_embeddings}/{len(satellite_images)}æœŸé–“\")\n",
    "    \n",
    "    if successful_embeddings >= 2:\n",
    "        print(f\"   âœ… æ™‚ç³»åˆ—å¤‰åŒ–åˆ†æã«ååˆ†ãªåŸ‹ã‚è¾¼ã¿ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ\")\n",
    "        embeddings_ready = True\n",
    "        \n",
    "        # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜\n",
    "        globals()['embeddings_data'] = embeddings_data\n",
    "        globals()['embeddings_ready'] = embeddings_ready\n",
    "        \n",
    "    else:\n",
    "        print(f\"   âš ï¸ æ™‚ç³»åˆ—åˆ†æã«ã¯æœ€ä½2æœŸé–“ã®åŸ‹ã‚è¾¼ã¿ãŒå¿…è¦ã§ã™\")\n",
    "        embeddings_ready = False\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ åˆ†æç”¨ã®è¡›æ˜Ÿç”»åƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "    print(\"ğŸ”§ ã¾ãšä¸Šã®ã‚»ãƒ«ã§ç”»åƒåé›†ã‚’æˆåŠŸã•ã›ã¦ãã ã•ã„\")\n",
    "    embeddings_ready = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed2003",
   "metadata": {},
   "source": [
    "## ğŸ“Š åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«åˆ†æã¨å¯è¦–åŒ–\n",
    "\n",
    "### ç”Ÿæˆã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ã®ç‰¹æ€§åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlphaEarthåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®è©³ç´°åˆ†æã¨å¯è¦–åŒ–\n",
    "if embeddings_ready and 'embeddings_data' in globals():\n",
    "    print(\"ğŸ“Š AlphaEarthåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«åˆ†æé–‹å§‹...\")\n",
    "    \n",
    "    # æˆåŠŸã—ãŸåŸ‹ã‚è¾¼ã¿ã®ã¿ã‚’æŠ½å‡º\n",
    "    successful_embeddings = {\n",
    "        period: data for period, data in embeddings_data.items() \n",
    "        if data.get('status') == 'success'\n",
    "    }\n",
    "    \n",
    "    if len(successful_embeddings) >= 2:\n",
    "        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’é…åˆ—ã¨ã—ã¦æº–å‚™\n",
    "        embedding_vectors = {}\n",
    "        embedding_labels = []\n",
    "        embedding_matrix = []\n",
    "        \n",
    "        for period_name, embedding_data in successful_embeddings.items():\n",
    "            vector = np.array(embedding_data['embedding_vector'])\n",
    "            embedding_vectors[period_name] = vector\n",
    "            embedding_labels.append(period_name)\n",
    "            embedding_matrix.append(vector)\n",
    "        \n",
    "        embedding_matrix = np.array(embedding_matrix)\n",
    "        \n",
    "        # å¯è¦–åŒ–ç”¨ã®å›³ä½œæˆ\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        fig.suptitle('ğŸ§  AlphaEarthåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«åˆ†æçµæœ', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®åˆ†å¸ƒæ¯”è¼ƒ\n",
    "        ax1 = axes[0, 0]\n",
    "        colors = ['blue', 'red', 'green']\n",
    "        period_names_jp = ['ç«ç½å‰', 'ç«ç½ä¸­', 'ç«ç½å¾Œ']\n",
    "        \n",
    "        for i, (period, vector) in enumerate(embedding_vectors.items()):\n",
    "            # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ è¡¨ç¤ºï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼šæœ€åˆã®50æ¬¡å…ƒï¼‰\n",
    "            ax1.hist(vector[:50], bins=20, alpha=0.6, \n",
    "                    color=colors[i], label=period_names_jp[i] if i < len(period_names_jp) else period)\n",
    "        \n",
    "        ax1.set_title('åŸ‹ã‚è¾¼ã¿å€¤åˆ†å¸ƒæ¯”è¼ƒï¼ˆæœ€åˆã®50æ¬¡å…ƒï¼‰', fontweight='bold')\n",
    "        ax1.set_xlabel('åŸ‹ã‚è¾¼ã¿å€¤')\n",
    "        ax1.set_ylabel('é »åº¦')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. æœŸé–“åˆ¥åŸ‹ã‚è¾¼ã¿çµ±è¨ˆ\n",
    "        ax2 = axes[0, 1]\n",
    "        stats_data = []\n",
    "        \n",
    "        for period, vector in embedding_vectors.items():\n",
    "            stats_data.append({\n",
    "                'Period': period,\n",
    "                'Mean': np.mean(vector),\n",
    "                'Std': np.std(vector),\n",
    "                'L2_Norm': np.linalg.norm(vector),\n",
    "                'Min': np.min(vector),\n",
    "                'Max': np.max(vector)\n",
    "            })\n",
    "        \n",
    "        stats_df = pd.DataFrame(stats_data)\n",
    "        \n",
    "        # çµ±è¨ˆå€¤ã®å¯è¦–åŒ–\n",
    "        x_pos = np.arange(len(stats_df))\n",
    "        width = 0.2\n",
    "        \n",
    "        ax2.bar(x_pos - width, stats_df['Mean'], width, label='å¹³å‡å€¤', alpha=0.8)\n",
    "        ax2.bar(x_pos, stats_df['Std'], width, label='æ¨™æº–åå·®', alpha=0.8)\n",
    "        ax2.bar(x_pos + width, stats_df['L2_Norm'], width, label='L2ãƒãƒ«ãƒ ', alpha=0.8)\n",
    "        \n",
    "        ax2.set_title('æœŸé–“åˆ¥åŸ‹ã‚è¾¼ã¿çµ±è¨ˆ', fontweight='bold')\n",
    "        ax2.set_xlabel('åˆ†ææœŸé–“')\n",
    "        ax2.set_ylabel('çµ±è¨ˆå€¤')\n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels([period_names_jp[i] if i < len(period_names_jp) else period \n",
    "                            for i, period in enumerate(stats_df['Period'])])\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹\n",
    "        ax3 = axes[0, 2]\n",
    "        \n",
    "        # é¡ä¼¼åº¦è¨ˆç®—\n",
    "        similarity_matrix = cosine_similarity(embedding_matrix)\n",
    "        \n",
    "        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º\n",
    "        im = ax3.imshow(similarity_matrix, cmap='RdYlBu_r', vmin=0, vmax=1)\n",
    "        \n",
    "        # ãƒ©ãƒ™ãƒ«è¨­å®š\n",
    "        period_labels = [period_names_jp[i] if i < len(period_names_jp) else period \n",
    "                        for i, period in enumerate(embedding_labels)]\n",
    "        ax3.set_xticks(range(len(period_labels)))\n",
    "        ax3.set_yticks(range(len(period_labels)))\n",
    "        ax3.set_xticklabels(period_labels)\n",
    "        ax3.set_yticklabels(period_labels)\n",
    "        \n",
    "        # é¡ä¼¼åº¦å€¤ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¡¨ç¤º\\n        for i in range(len(similarity_matrix)):\\n            for j in range(len(similarity_matrix)):\\n                text = ax3.text(j, i, f'{similarity_matrix[i, j]:.3f}',\\n                               ha=\\\"center\\\", va=\\\"center\\\", color=\\\"black\\\", fontweight='bold')\\n        \\n        ax3.set_title('æœŸé–“é–“ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦', fontweight='bold')\\n        plt.colorbar(im, ax=ax3, label='é¡ä¼¼åº¦')\\n        \\n        # 4. åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒé‡è¦åº¦åˆ†æï¼ˆåˆ†æ•£ãƒ™ãƒ¼ã‚¹ï¼‰\\n        ax4 = axes[1, 0]\\n        \\n        # å„æ¬¡å…ƒã®æœŸé–“é–“åˆ†æ•£ã‚’è¨ˆç®—\\n        dimension_variances = np.var(embedding_matrix, axis=0)\\n        top_dimensions = np.argsort(dimension_variances)[-20:]  # ä¸Šä½20æ¬¡å…ƒ\\n        \\n        ax4.bar(range(len(top_dimensions)), dimension_variances[top_dimensions], alpha=0.7)\\n        ax4.set_title('é«˜åˆ†æ•£æ¬¡å…ƒï¼ˆä¸Šä½20æ¬¡å…ƒï¼‰', fontweight='bold')\\n        ax4.set_xlabel('æ¬¡å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹')\\n        ax4.set_ylabel('åˆ†æ•£å€¤')\\n        ax4.grid(True, alpha=0.3)\\n        \\n        # 5. PCAé™æ¬¡å…ƒå¯è¦–åŒ–\\n        ax5 = axes[1, 1]\\n        \\n        from sklearn.decomposition import PCA\\n        \\n        # PCAã§2æ¬¡å…ƒã«å‰Šæ¸›\\n        pca = PCA(n_components=2)\\n        embedding_2d = pca.fit_transform(embedding_matrix)\\n        \\n        # æœŸé–“åˆ¥ã«è‰²åˆ†ã‘ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆ\\n        for i, (period, vector_2d) in enumerate(zip(embedding_labels, embedding_2d)):\\n            period_label = period_names_jp[i] if i < len(period_names_jp) else period\\n            ax5.scatter(vector_2d[0], vector_2d[1], \\n                       c=colors[i], s=200, alpha=0.8, \\n                       label=period_label, edgecolors='black', linewidth=2)\\n            \\n            # ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ \\n            ax5.annotate(period_label, (vector_2d[0], vector_2d[1]), \\n                        xytext=(5, 5), textcoords='offset points', \\n                        fontsize=10, fontweight='bold')\\n        \\n        ax5.set_title(f'PCA 2æ¬¡å…ƒå¯è¦–åŒ–\\\\n(å¯„ä¸ç‡: PC1={pca.explained_variance_ratio_[0]:.1%}, PC2={pca.explained_variance_ratio_[1]:.1%})', \\n                     fontweight='bold')\\n        ax5.set_xlabel(f'ç¬¬1ä¸»æˆåˆ† ({pca.explained_variance_ratio_[0]:.1%})')\\n        ax5.set_ylabel(f'ç¬¬2ä¸»æˆåˆ† ({pca.explained_variance_ratio_[1]:.1%})')\\n        ax5.legend()\\n        ax5.grid(True, alpha=0.3)\\n        \\n        # 6. åˆ†æã‚µãƒãƒªãƒ¼\\n        ax6 = axes[1, 2]\\n        ax6.axis('off')\\n        \\n        # æœ€ã‚‚å¤‰åŒ–ã®å¤§ãã„æœŸé–“ãƒšã‚¢ã‚’ç‰¹å®š\\n        min_similarity = np.min(similarity_matrix + np.eye(len(similarity_matrix)))  # å¯¾è§’ç·šé™¤å¤–\\n        max_change_indices = np.unravel_index(np.argmin(similarity_matrix + np.eye(len(similarity_matrix))), \\n                                             similarity_matrix.shape)\\n        \\n        period1 = embedding_labels[max_change_indices[0]]\\n        period2 = embedding_labels[max_change_indices[1]]\\n        period1_jp = period_names_jp[max_change_indices[0]] if max_change_indices[0] < len(period_names_jp) else period1\\n        period2_jp = period_names_jp[max_change_indices[1]] if max_change_indices[1] < len(period_names_jp) else period2\\n        \\n        # PCAå¯„ä¸ç‡\\n        total_variance_explained = sum(pca.explained_variance_ratio_)\\n        \\n        summary_text = f\\\"\\\"\\\"\\nğŸ§  AlphaEarthåŸ‹ã‚è¾¼ã¿åˆ†æã‚µãƒãƒªãƒ¼\\n\\nğŸ“Š åŸºæœ¬æƒ…å ±:\\n   â€¢ åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {embedding_matrix.shape[1]}\\n   â€¢ åˆ†ææœŸé–“æ•°: {len(successful_embeddings)}\\n   â€¢ å‡¦ç†æ–¹æ³•: {list(successful_embeddings.values())[0]['processing_method']}\\n\\nğŸ” é¡ä¼¼åº¦åˆ†æ:\\n   â€¢ æœ€å°é¡ä¼¼åº¦: {min_similarity:.3f}\\n   â€¢ æœ€å¤§å¤‰åŒ–ãƒšã‚¢: {period1_jp} â†” {period2_jp}\\n   \\nğŸ“ˆ ä¸»æˆåˆ†åˆ†æ:\\n   â€¢ 2æ¬¡å…ƒå¯„ä¸ç‡: {total_variance_explained:.1%}\\n   â€¢ æœ€å¤§åˆ†æ•£æ¬¡å…ƒ: {top_dimensions[-1]}ç•ªç›®\\n\\nğŸ¯ å¤‰åŒ–æ¤œçŸ¥çµæœ:\\n   {'âœ… æ˜ç¢ºãªæ™‚ç³»åˆ—å¤‰åŒ–ã‚’æ¤œå‡º' if min_similarity < 0.8 else 'âš ï¸ å¤‰åŒ–ãŒé™å®šçš„'}\\n        \\\"\\\"\\\"\\n        \\n        ax6.text(0.05, 0.95, summary_text, transform=ax6.transAxes, fontsize=11,\\n                verticalalignment='top', \\n                bbox=dict(boxstyle=\\\"round,pad=0.5\\\", facecolor=\\\"lightgreen\\\", alpha=0.8))\\n        \\n        plt.tight_layout()\\n        plt.show()\\n        \\n        print(\\\"âœ… AlphaEarthåŸ‹ã‚è¾¼ã¿åˆ†æå®Œäº†\\\")\\n        print(f\\\"ğŸ“Š {len(successful_embeddings)}æœŸé–“ã®åŸ‹ã‚è¾¼ã¿ã‚’åˆ†æ\\\")\\n        print(f\\\"ğŸ§  æ¬¡å…ƒæ•°: {embedding_matrix.shape[1]}\\\")\\n        print(f\\\"ğŸ¯ é¡ä¼¼åº¦åˆ†ææº–å‚™å®Œäº†\\\")\\n        \\n        # åˆ†æçµæœã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜\\n        globals()['embedding_analysis'] = {\\n            'vectors': embedding_vectors,\\n            'matrix': embedding_matrix,\\n            'similarity_matrix': similarity_matrix,\\n            'pca_result': embedding_2d,\\n            'pca_model': pca,\\n            'statistics': stats_df\\n        }\\n        \\n    else:\\n        print(\\\"âš ï¸ åˆ†æã«ã¯æœ€ä½2æœŸé–“ã®æˆåŠŸã—ãŸåŸ‹ã‚è¾¼ã¿ãŒå¿…è¦ã§ã™\\\")\\nelse:\\n    print(\\\"âš ï¸ åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\\\")\\n    print(\\\"ğŸ”§ ã¾ãšä¸Šã®ã‚»ãƒ«ã§åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚’æˆåŠŸã•ã›ã¦ãã ã•ã„\\\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ed880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®è©³ç´°æƒ…å ±ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§è¡¨ç¤º\n",
    "if embeddings_ready and 'embeddings_data' in globals():\n",
    "    print(\\\"ğŸ“‹ AlphaEarthåŸ‹ã‚è¾¼ã¿è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ\\\")\n",
    "    print(\\\"=\\\"*70)\n",
    "    \n",
    "    # è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ\n",
    "    detailed_report = []\\n    \n",
    "    for period_name, embedding_data in embeddings_data.items():\n",
    "        if embedding_data.get('status') == 'success':\n",
    "            vector = np.array(embedding_data['embedding_vector'])\n",
    "            \n",
    "            # æœŸé–“åã®æ—¥æœ¬èªå¤‰æ›\n",
    "            period_translations = {\n",
    "                'pre_fire': 'ç«ç½ç™ºç”Ÿå‰',\n",
    "                'during_fire': 'ç«ç½ç™ºç”Ÿä¸­', \n",
    "                'post_fire': 'ç«ç½ç™ºç”Ÿå¾Œ'\n",
    "            }\n",
    "            period_jp = period_translations.get(period_name, period_name)\n",
    "            \n",
    "            detailed_report.append({\n",
    "                'æœŸé–“': period_jp,\n",
    "                'æœŸé–“ã‚³ãƒ¼ãƒ‰': period_name,\n",
    "                'æ¬¡å…ƒæ•°': embedding_data['embedding_dimension'],\n",
    "                'å¹³å‡å€¤': f\\\"{np.mean(vector):.4f}\\\",\n",
    "                'æ¨™æº–åå·®': f\\\"{np.std(vector):.4f}\\\",\n",
    "                'L2ãƒãƒ«ãƒ ': f\\\"{np.linalg.norm(vector):.4f}\\\",\n",
    "                'æœ€å°å€¤': f\\\"{np.min(vector):.4f}\\\",\n",
    "                'æœ€å¤§å€¤': f\\\"{np.max(vector):.4f}\\\",\n",
    "                'å‡¦ç†æ–¹æ³•': embedding_data['processing_method'],\n",
    "                'ç”Ÿæˆæ™‚åˆ»': embedding_data['generation_timestamp'][:19].replace('T', ' ')\n",
    "            })\n",
    "    \n",
    "    if detailed_report:\n",
    "        # DataFrameã¨ã—ã¦è¡¨ç¤º\n",
    "        report_df = pd.DataFrame(detailed_report)\n",
    "        \n",
    "        print(\\\"\\\\nğŸ“Š æœŸé–“åˆ¥åŸ‹ã‚è¾¼ã¿çµ±è¨ˆã‚µãƒãƒªãƒ¼:\\\")\n",
    "        display(report_df)\n",
    "        \n",
    "        # é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è©³ç´°\n",
    "        if 'embedding_analysis' in globals():\n",
    "            similarity_matrix = embedding_analysis['similarity_matrix']\n",
    "            period_codes = [item['æœŸé–“ã‚³ãƒ¼ãƒ‰'] for item in detailed_report]\n",
    "            period_names = [item['æœŸé–“'] for item in detailed_report]\n",
    "            \n",
    "            print(\\\"\\\\nğŸ” æœŸé–“é–“é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹:\\\")\n",
    "            similarity_df = pd.DataFrame(\n",
    "                similarity_matrix, \n",
    "                index=period_names, \n",
    "                columns=period_names\n",
    "            )\n",
    "            display(similarity_df.round(4))\n",
    "            \n",
    "            # å¤‰åŒ–ã®å¤§ãã•ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "            print(\\\"\\\\nğŸ“ˆ æœŸé–“é–“å¤‰åŒ–ã®å¤§ãã•ãƒ©ãƒ³ã‚­ãƒ³ã‚°:\\\")\n",
    "            changes = []\n",
    "            for i in range(len(period_codes)):\n",
    "                for j in range(i+1, len(period_codes)):\n",
    "                    similarity = similarity_matrix[i, j]\n",
    "                    change_magnitude = 1 - similarity  # å¤‰åŒ–ã®å¤§ãã•\n",
    "                    changes.append({\n",
    "                        'æœŸé–“ãƒšã‚¢': f\\\"{period_names[i]} â†’ {period_names[j]}\\\",\n",
    "                        'é¡ä¼¼åº¦': f\\\"{similarity:.4f}\\\",\n",
    "                        'å¤‰åŒ–ã®å¤§ãã•': f\\\"{change_magnitude:.4f}\\\",\n",
    "                        'å¤‰åŒ–ãƒ¬ãƒ™ãƒ«': 'HIGH' if change_magnitude > 0.3 else 'MEDIUM' if change_magnitude > 0.1 else 'LOW'\n",
    "                    })\n",
    "            \n",
    "            changes_df = pd.DataFrame(changes).sort_values('å¤‰åŒ–ã®å¤§ãã•', ascending=False)\n",
    "            display(changes_df)\n",
    "            \n",
    "            print(\\\"\\\\nğŸ¯ ç«ç½æ¤œçŸ¥åˆ†æçµæœ:\\\")\n",
    "            max_change = changes_df.iloc[0]['å¤‰åŒ–ã®å¤§ãã•']\n",
    "            max_change_pair = changes_df.iloc[0]['æœŸé–“ãƒšã‚¢']\n",
    "            \n",
    "            if float(max_change) > 0.3:\n",
    "                detection_result = \\\"ğŸ”¥ æ˜ç¢ºãªç«ç½ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º\\\"\\n                confidence = \\\"HIGH\\\"\\n            elif float(max_change) > 0.1:\\n                detection_result = \\\"âš ï¸ ä¸­ç¨‹åº¦ã®å¤‰åŒ–ã‚’æ¤œå‡º\\\"\\n                confidence = \\\"MEDIUM\\\"\\n            else:\\n                detection_result = \\\"âœ… å®‰å®šã—ãŸçŠ¶æ…‹ã‚’ç¢ºèª\\\"\\n                confidence = \\\"LOW\\\"\\n            \\n            print(f\\\"   æ¤œçŸ¥çµæœ: {detection_result}\\\")\\n            print(f\\\"   ä¿¡é ¼åº¦: {confidence}\\\")\\n            print(f\\\"   æœ€å¤§å¤‰åŒ–: {max_change_pair} ({max_change})\\\")\\n            \\n        print(\\\"\\\\nâœ… AlphaEarthåŸ‹ã‚è¾¼ã¿è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆå®Œäº†\\\")\\n    else:\\n        print(\\\"âš ï¸ æˆåŠŸã—ãŸåŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\\\")\\nelse:\\n    print(\\\"âš ï¸ åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\\\")\\n    print(\\\"ğŸ”§ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2557f64",
   "metadata": {},
   "source": [
    "## ğŸ” é«˜åº¦ãªé¡ä¼¼åº¦ãƒ»å·®åˆ†åˆ†æã‚·ã‚¹ãƒ†ãƒ \n",
    "\n",
    "### AlphaEarthåŸ‹ã‚è¾¼ã¿æ™‚ç³»åˆ—å¤‰åŒ–åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "import networkx as nx\n",
    "\n",
    "class AdvancedSimilarityAnalyzer:\n",
    "    \"\"\"é«˜åº¦ãªé¡ä¼¼åº¦ãƒ»å·®åˆ†åˆ†æã‚·ã‚¹ãƒ†ãƒ \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_vectors, fire_threshold_config=None):\n",
    "        \"\"\"\n",
    "        é«˜åº¦åˆ†æã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–\n",
    "        \n",
    "        Args:\n",
    "            embedding_vectors: æœŸé–“åˆ¥åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«è¾æ›¸\n",
    "            fire_threshold_config: ç«ç½æ¤œçŸ¥é–¾å€¤è¨­å®š\n",
    "        \"\"\"\n",
    "        self.embedding_vectors = embedding_vectors\n",
    "        self.embedding_matrix = np.array(list(embedding_vectors.values()))\n",
    "        self.period_names = list(embedding_vectors.keys())\n",
    "        \n",
    "        # ç«ç½æ¤œçŸ¥é–¾å€¤è¨­å®š\n",
    "        self.fire_thresholds = fire_threshold_config or {\n",
    "            'high_change': 0.3,      # é«˜ã„å¤‰åŒ–ï¼ˆæ˜ç¢ºãªç«ç½ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰\n",
    "            'medium_change': 0.15,   # ä¸­ç¨‹åº¦ã®å¤‰åŒ–\n",
    "            'low_change': 0.05,      # ä½ã„å¤‰åŒ–\n",
    "            'anomaly_score': 0.8,    # ç•°å¸¸ã‚¹ã‚³ã‚¢é–¾å€¤\n",
    "            'cluster_separation': 0.7 # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†é›¢åº¦\n",
    "        }\n",
    "        \n",
    "        print(\"ğŸ” é«˜åº¦ãªé¡ä¼¼åº¦ãƒ»å·®åˆ†åˆ†æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†\")\n",
    "        \n",
    "    def comprehensive_similarity_analysis(self):\n",
    "        \"\"\"åŒ…æ‹¬çš„é¡ä¼¼åº¦åˆ†æã‚’å®Ÿè¡Œ\"\"\"\n",
    "        \n",
    "        print(\"ğŸ§® åŒ…æ‹¬çš„é¡ä¼¼åº¦åˆ†æé–‹å§‹...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        analysis_results = {\n",
    "            'basic_similarities': {},\n",
    "            'distance_metrics': {},\n",
    "            'temporal_analysis': {},\n",
    "            'anomaly_detection': {},\n",
    "            'clustering_analysis': {},\n",
    "            'change_patterns': {}\n",
    "        }\n",
    "        \n",
    "        # 1. åŸºæœ¬é¡ä¼¼åº¦è¨ˆç®—\n",
    "        print(\"\\nğŸ“Š åŸºæœ¬é¡ä¼¼åº¦ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹è¨ˆç®—ä¸­...\")\n",
    "        analysis_results['basic_similarities'] = self._calculate_basic_similarities()\n",
    "        \n",
    "        # 2. è·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹åˆ†æ\n",
    "        print(\"ğŸ“ å¤šæ§˜ãªè·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹åˆ†æä¸­...\")\n",
    "        analysis_results['distance_metrics'] = self._calculate_distance_metrics()\n",
    "        \n",
    "        # 3. æ™‚ç³»åˆ—å¤‰åŒ–åˆ†æ\n",
    "        print(\"â° æ™‚ç³»åˆ—å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æä¸­...\")\n",
    "        analysis_results['temporal_analysis'] = self._analyze_temporal_patterns()\n",
    "        \n",
    "        # 4. ç•°å¸¸æ¤œçŸ¥åˆ†æ\n",
    "        print(\"ğŸš¨ ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè¡Œä¸­...\")\n",
    "        analysis_results['anomaly_detection'] = self._detect_anomalies()\n",
    "        \n",
    "        # 5. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ\n",
    "        print(\"ğŸ”— ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æå®Ÿè¡Œä¸­...\")\n",
    "        analysis_results['clustering_analysis'] = self._perform_clustering_analysis()\n",
    "        \n",
    "        # 6. å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®š\n",
    "        print(\"ğŸ“ˆ å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®šä¸­...\")\n",
    "        analysis_results['change_patterns'] = self._identify_change_patterns()\n",
    "        \n",
    "        print(\"\\nâœ… åŒ…æ‹¬çš„é¡ä¼¼åº¦åˆ†æå®Œäº†\")\n",
    "        return analysis_results\n",
    "    \n",
    "    def _calculate_basic_similarities(self):\n",
    "        \"\"\"åŸºæœ¬çš„ãªé¡ä¼¼åº¦ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n",
    "        cosine_sim = cosine_similarity(self.embedding_matrix)\n",
    "        results['cosine_similarity'] = cosine_sim\n",
    "        \n",
    "        # ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°\n",
    "        correlation_matrix = np.corrcoef(self.embedding_matrix)\n",
    "        results['pearson_correlation'] = correlation_matrix\n",
    "        \n",
    "        # ã‚¹ãƒ”ã‚¢ãƒãƒ³é †ä½ç›¸é–¢\n",
    "        spearman_corr = np.zeros((len(self.embedding_matrix), len(self.embedding_matrix)))\n",
    "        for i in range(len(self.embedding_matrix)):\n",
    "            for j in range(len(self.embedding_matrix)):\n",
    "                corr, _ = stats.spearmanr(self.embedding_matrix[i], self.embedding_matrix[j])\n",
    "                spearman_corr[i, j] = corr if not np.isnan(corr) else 0\n",
    "        results['spearman_correlation'] = spearman_corr\n",
    "        \n",
    "        # æ­£è¦åŒ–ç›¸äº’æƒ…å ±é‡ï¼ˆè¿‘ä¼¼ï¼‰\n",
    "        mutual_info = self._calculate_mutual_information()\n",
    "        results['mutual_information'] = mutual_info\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_distance_metrics(self):\n",
    "        \"\"\"å¤šæ§˜ãªè·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢\n",
    "        euclidean_dist = squareform(pdist(self.embedding_matrix, metric='euclidean'))\n",
    "        results['euclidean_distance'] = euclidean_dist\n",
    "        \n",
    "        # ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢\n",
    "        manhattan_dist = squareform(pdist(self.embedding_matrix, metric='manhattan'))\n",
    "        results['manhattan_distance'] = manhattan_dist\n",
    "        \n",
    "        # ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•è·é›¢\n",
    "        chebyshev_dist = squareform(pdist(self.embedding_matrix, metric='chebyshev'))\n",
    "        results['chebyshev_distance'] = chebyshev_dist\n",
    "        \n",
    "        # ãƒŸãƒ³ã‚³ãƒ•ã‚¹ã‚­ãƒ¼è·é›¢ï¼ˆp=3ï¼‰\n",
    "        minkowski_dist = squareform(pdist(self.embedding_matrix, metric='minkowski', p=3))\n",
    "        results['minkowski_distance'] = minkowski_dist\n",
    "        \n",
    "        # ãƒ¯ãƒƒã‚µãƒ¼ã‚¹ã‚¿ã‚¤ãƒ³è·é›¢ï¼ˆ1æ¬¡å…ƒå°„å½±ã«ã‚ˆã‚‹è¿‘ä¼¼ï¼‰\n",
    "        wasserstein_dist = self._calculate_wasserstein_distance()\n",
    "        results['wasserstein_distance'] = wasserstein_dist\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _analyze_temporal_patterns(self):\n",
    "        \"\"\"æ™‚ç³»åˆ—å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # æœŸé–“é †åºã®è¨­å®š\n",
    "        period_order = ['pre_fire', 'during_fire', 'post_fire']\n",
    "        ordered_periods = [p for p in period_order if p in self.period_names]\n",
    "        \n",
    "        if len(ordered_periods) >= 2:\n",
    "            # é€£ç¶šæœŸé–“é–“ã®å¤‰åŒ–é‡è¨ˆç®—\n",
    "            sequential_changes = []\n",
    "            for i in range(len(ordered_periods) - 1):\n",
    "                period1 = ordered_periods[i]\n",
    "                period2 = ordered_periods[i + 1]\n",
    "                \n",
    "                if period1 in self.embedding_vectors and period2 in self.embedding_vectors:\n",
    "                    vec1 = self.embedding_vectors[period1]\n",
    "                    vec2 = self.embedding_vectors[period2]\n",
    "                    \n",
    "                    # è¤‡æ•°ã®å¤‰åŒ–æŒ‡æ¨™ã‚’è¨ˆç®—\n",
    "                    cosine_change = 1 - cosine_similarity([vec1], [vec2])[0, 0]\n",
    "                    euclidean_change = np.linalg.norm(vec1 - vec2)\n",
    "                    relative_change = euclidean_change / (np.linalg.norm(vec1) + np.linalg.norm(vec2) + 1e-8)\n",
    "                    \n",
    "                    sequential_changes.append({\n",
    "                        'transition': f'{period1} â†’ {period2}',\n",
    "                        'cosine_change': cosine_change,\n",
    "                        'euclidean_change': euclidean_change,\n",
    "                        'relative_change': relative_change\n",
    "                    })\n",
    "            \n",
    "            results['sequential_changes'] = sequential_changes\n",
    "            \n",
    "            # å¤‰åŒ–åŠ é€Ÿåº¦åˆ†æ\n",
    "            if len(sequential_changes) >= 2:\n",
    "                change_acceleration = []\n",
    "                for i in range(len(sequential_changes) - 1):\n",
    "                    current_change = sequential_changes[i]['cosine_change']\n",
    "                    next_change = sequential_changes[i + 1]['cosine_change']\n",
    "                    acceleration = next_change - current_change\n",
    "                    \n",
    "                    change_acceleration.append({\n",
    "                        'period': f\"Phase {i+1} â†’ Phase {i+2}\",\n",
    "                        'acceleration': acceleration,\n",
    "                        'change_trend': 'accelerating' if acceleration > 0.05 else 'decelerating' if acceleration < -0.05 else 'stable'\n",
    "                    })\n",
    "                \n",
    "                results['change_acceleration'] = change_acceleration\n",
    "        \n",
    "        # å¤‰åŒ–æ–¹å‘æ€§åˆ†æï¼ˆãƒ™ã‚¯ãƒˆãƒ«è§’åº¦ï¼‰\n",
    "        direction_analysis = self._analyze_change_directions()\n",
    "        results['direction_analysis'] = direction_analysis\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _detect_anomalies(self):\n",
    "        \"\"\"é«˜åº¦ãªç•°å¸¸æ¤œçŸ¥ã‚’å®Ÿè¡Œ\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. çµ±è¨ˆçš„ç•°å¸¸æ¤œçŸ¥ï¼ˆZ-score ãƒ™ãƒ¼ã‚¹ï¼‰\n",
    "        embedding_means = np.mean(self.embedding_matrix, axis=0)\n",
    "        embedding_stds = np.std(self.embedding_matrix, axis=0)\n",
    "        \n",
    "        z_scores = []\n",
    "        for i, (period, vector) in enumerate(self.embedding_vectors.items()):\n",
    "            z_score = np.abs((vector - embedding_means) / (embedding_stds + 1e-8))\n",
    "            anomaly_score = np.mean(z_score)\n",
    "            \n",
    "            z_scores.append({\n",
    "                'period': period,\n",
    "                'anomaly_score': anomaly_score,\n",
    "                'is_anomaly': anomaly_score > self.fire_thresholds['anomaly_score'],\n",
    "                'top_anomalous_dimensions': np.argsort(z_score)[-10:].tolist()\n",
    "            })\n",
    "        \n",
    "        results['statistical_anomalies'] = z_scores\n",
    "        \n",
    "        # 2. å­¤ç«‹æ£®æ—ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        \n",
    "        if len(self.embedding_matrix) >= 2:\n",
    "            iso_forest = IsolationForest(contamination=0.3, random_state=42)\n",
    "            anomaly_labels = iso_forest.fit_predict(self.embedding_matrix)\n",
    "            anomaly_scores = iso_forest.score_samples(self.embedding_matrix)\n",
    "            \n",
    "            isolation_results = []\n",
    "            for i, (period, score, label) in enumerate(zip(self.period_names, anomaly_scores, anomaly_labels)):\n",
    "                isolation_results.append({\n",
    "                    'period': period,\n",
    "                    'isolation_score': score,\n",
    "                    'is_outlier': label == -1,\n",
    "                    'normalized_score': (score - np.min(anomaly_scores)) / (np.max(anomaly_scores) - np.min(anomaly_scores) + 1e-8)\n",
    "                })\n",
    "            \n",
    "            results['isolation_forest'] = isolation_results\n",
    "        \n",
    "        # 3. LOFï¼ˆLocal Outlier Factorï¼‰ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥\n",
    "        from sklearn.neighbors import LocalOutlierFactor\n",
    "        \n",
    "        if len(self.embedding_matrix) >= 3:\n",
    "            lof = LocalOutlierFactor(n_neighbors=min(2, len(self.embedding_matrix)-1), contamination=0.3)\n",
    "            lof_labels = lof.fit_predict(self.embedding_matrix)\n",
    "            lof_scores = lof.negative_outlier_factor_\n",
    "            \n",
    "            lof_results = []\n",
    "            for i, (period, score, label) in enumerate(zip(self.period_names, lof_scores, lof_labels)):\n",
    "                lof_results.append({\n",
    "                    'period': period,\n",
    "                    'lof_score': score,\n",
    "                    'is_outlier': label == -1,\n",
    "                    'outlier_strength': abs(score) if score < -1 else 0\n",
    "                })\n",
    "            \n",
    "            results['local_outlier_factor'] = lof_results\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _perform_clustering_analysis(self):\n",
    "        \"\"\"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æã‚’å®Ÿè¡Œ\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. K-means ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°\n",
    "        if len(self.embedding_matrix) >= 2:\n",
    "            optimal_k = min(3, len(self.embedding_matrix))\n",
    "            \n",
    "            # æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®æ¢ç´¢ï¼ˆã‚·ãƒ«ã‚¨ãƒƒãƒˆåˆ†æï¼‰\n",
    "            silhouette_scores = []\n",
    "            k_range = range(2, min(len(self.embedding_matrix) + 1, 5))\n",
    "            \n",
    "            for k in k_range:\n",
    "                if k <= len(self.embedding_matrix):\n",
    "                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "                    cluster_labels = kmeans.fit_predict(self.embedding_matrix)\n",
    "                    if len(set(cluster_labels)) > 1:  # è¤‡æ•°ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿\n",
    "                        silhouette_avg = silhouette_score(self.embedding_matrix, cluster_labels)\n",
    "                        silhouette_scores.append({'k': k, 'silhouette_score': silhouette_avg})\n",
    "            \n",
    "            if silhouette_scores:\n",
    "                best_k = max(silhouette_scores, key=lambda x: x['silhouette_score'])['k']\n",
    "                \n",
    "                # æœ€é©Kã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ\n",
    "                kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "                cluster_labels = kmeans.fit_predict(self.embedding_matrix)\n",
    "                cluster_centers = kmeans.cluster_centers_\n",
    "                \n",
    "                clustering_results = []\n",
    "                for i, (period, label) in enumerate(zip(self.period_names, cluster_labels)):\n",
    "                    distance_to_center = np.linalg.norm(self.embedding_matrix[i] - cluster_centers[label])\n",
    "                    clustering_results.append({\n",
    "                        'period': period,\n",
    "                        'cluster': int(label),\n",
    "                        'distance_to_center': distance_to_center,\n",
    "                        'cluster_size': np.sum(cluster_labels == label)\n",
    "                    })\n",
    "                \n",
    "                results['kmeans'] = {\n",
    "                    'optimal_k': best_k,\n",
    "                    'silhouette_scores': silhouette_scores,\n",
    "                    'cluster_assignments': clustering_results,\n",
    "                    'cluster_centers': cluster_centers.tolist()\n",
    "                }\n",
    "        \n",
    "        # 2. éšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°\n",
    "        from scipy.cluster.hierarchy import linkage, fcluster\n",
    "        from scipy.spatial.distance import pdist\n",
    "        \n",
    "        if len(self.embedding_matrix) >= 2:\n",
    "            # è·é›¢è¡Œåˆ—è¨ˆç®—\n",
    "            distances = pdist(self.embedding_matrix, metric='cosine')\n",
    "            \n",
    "            # éšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ\n",
    "            linkage_matrix = linkage(distances, method='ward')\n",
    "            \n",
    "            # é©åˆ‡ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã§åˆ†å‰²\n",
    "            n_clusters = min(3, len(self.embedding_matrix))\n",
    "            hierarchical_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
    "            \n",
    "            hierarchical_results = []\n",
    "            for i, (period, label) in enumerate(zip(self.period_names, hierarchical_labels)):\n",
    "                hierarchical_results.append({\n",
    "                    'period': period,\n",
    "                    'cluster': int(label),\n",
    "                    'cluster_size': np.sum(hierarchical_labels == label)\n",
    "                })\n",
    "            \n",
    "            results['hierarchical'] = {\n",
    "                'cluster_assignments': hierarchical_results,\n",
    "                'linkage_matrix': linkage_matrix.tolist(),\n",
    "                'n_clusters': n_clusters\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _identify_change_patterns(self):\n",
    "        \"\"\"å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. å¤‰åŒ–ã®æ–¹å‘æ€§åˆ†æ\n",
    "        if len(self.embedding_matrix) >= 2:\n",
    "            # ä¸»æˆåˆ†åˆ†æã«ã‚ˆã‚‹å¤‰åŒ–æ–¹å‘ã®ç‰¹å®š\n",
    "            from sklearn.decomposition import PCA\n",
    "            \n",
    "            pca = PCA(n_components=min(3, self.embedding_matrix.shape[1]))\n",
    "            embedded_pca = pca.fit_transform(self.embedding_matrix)\n",
    "            \n",
    "            # æ™‚ç³»åˆ—é †ã§ã®å¤‰åŒ–ãƒ™ã‚¯ãƒˆãƒ«\n",
    "            period_order = ['pre_fire', 'during_fire', 'post_fire']\n",
    "            ordered_indices = [self.period_names.index(p) for p in period_order if p in self.period_names]\n",
    "            \n",
    "            if len(ordered_indices) >= 2:\n",
    "                change_vectors = []\n",
    "                for i in range(len(ordered_indices) - 1):\n",
    "                    idx1, idx2 = ordered_indices[i], ordered_indices[i + 1]\n",
    "                    change_vector = embedded_pca[idx2] - embedded_pca[idx1]\n",
    "                    magnitude = np.linalg.norm(change_vector)\n",
    "                    \n",
    "                    change_vectors.append({\n",
    "                        'transition': f'{period_order[i]} â†’ {period_order[i+1]}',\n",
    "                        'change_vector': change_vector.tolist(),\n",
    "                        'magnitude': magnitude,\n",
    "                        'dominant_component': np.argmax(np.abs(change_vector))\n",
    "                    })\n",
    "                \n",
    "                results['change_vectors'] = change_vectors\n",
    "                results['pca_explained_variance'] = pca.explained_variance_ratio_.tolist()\n",
    "        \n",
    "        # 2. ç«ç½ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®š\n",
    "        fire_pattern_analysis = self._analyze_fire_patterns()\n",
    "        results['fire_patterns'] = fire_pattern_analysis\n",
    "        \n",
    "        # 3. ç•°å¸¸åº¦ã‚¹ã‚³ã‚¢çµ±åˆ\n",
    "        anomaly_integration = self._integrate_anomaly_scores()\n",
    "        results['integrated_anomaly'] = anomaly_integration\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_mutual_information(self):\n",
    "        \"\"\"ç›¸äº’æƒ…å ±é‡ã‚’è¿‘ä¼¼è¨ˆç®—\"\"\"\n",
    "        n = len(self.embedding_matrix)\n",
    "        mi_matrix = np.zeros((n, n))\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i == j:\n",
    "                    mi_matrix[i, j] = 1.0\n",
    "                else:\n",
    "                    # ç°¡å˜ãªç›¸äº’æƒ…å ±é‡ã®è¿‘ä¼¼ï¼ˆãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ã®éç·šå½¢ç‰ˆï¼‰\n",
    "                    corr = np.corrcoef(self.embedding_matrix[i], self.embedding_matrix[j])[0, 1]\n",
    "                    mi_approx = -0.5 * np.log(1 - corr**2 + 1e-8)\n",
    "                    mi_matrix[i, j] = mi_approx\n",
    "        \n",
    "        return mi_matrix\n",
    "    \n",
    "    def _calculate_wasserstein_distance(self):\n",
    "        \"\"\"ãƒ¯ãƒƒã‚µãƒ¼ã‚¹ã‚¿ã‚¤ãƒ³è·é›¢ã‚’è¿‘ä¼¼è¨ˆç®—\"\"\"\n",
    "        from scipy.stats import wasserstein_distance\n",
    "        \n",
    "        n = len(self.embedding_matrix)\n",
    "        w_dist_matrix = np.zeros((n, n))\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i == j:\n",
    "                    w_dist_matrix[i, j] = 0.0\n",
    "                else:\n",
    "                    # å„æ¬¡å…ƒã§ã®1æ¬¡å…ƒãƒ¯ãƒƒã‚µãƒ¼ã‚¹ã‚¿ã‚¤ãƒ³è·é›¢ã®å¹³å‡\n",
    "                    distances = []\n",
    "                    for dim in range(min(50, self.embedding_matrix.shape[1])):  # è¨ˆç®—åŠ¹ç‡ã®ãŸã‚æœ€åˆã®50æ¬¡å…ƒã®ã¿\n",
    "                        dist = wasserstein_distance(\n",
    "                            [self.embedding_matrix[i, dim]], \n",
    "                            [self.embedding_matrix[j, dim]]\n",
    "                        )\n",
    "                        distances.append(dist)\n",
    "                    w_dist_matrix[i, j] = np.mean(distances)\n",
    "        \n",
    "        return w_dist_matrix\n",
    "    \n",
    "    def _analyze_change_directions(self):\n",
    "        \"\"\"å¤‰åŒ–æ–¹å‘æ€§ã‚’åˆ†æ\"\"\"\n",
    "        \n",
    "        if len(self.embedding_matrix) < 2:\n",
    "            return {}\n",
    "        \n",
    "        results = {}\n",
    "        period_pairs = []\n",
    "        \n",
    "        # å…¨ãƒšã‚¢çµ„ã¿åˆã‚ã›ã§ã®æ–¹å‘åˆ†æ\n",
    "        for i in range(len(self.period_names)):\n",
    "            for j in range(i + 1, len(self.period_names)):\n",
    "                vec1 = self.embedding_matrix[i]\n",
    "                vec2 = self.embedding_matrix[j]\n",
    "                \n",
    "                # å¤‰åŒ–ãƒ™ã‚¯ãƒˆãƒ«\n",
    "                change_vec = vec2 - vec1\n",
    "                change_magnitude = np.linalg.norm(change_vec)\n",
    "                \n",
    "                # è§’åº¦åˆ†æï¼ˆç¬¬1ä¸»æˆåˆ†ã¨ã®è§’åº¦ï¼‰\n",
    "                if change_magnitude > 0:\n",
    "                    # æ­£è¦åŒ–ã•ã‚ŒãŸå¤‰åŒ–ãƒ™ã‚¯ãƒˆãƒ«\n",
    "                    normalized_change = change_vec / change_magnitude\n",
    "                    \n",
    "                    # æœ€ã‚‚å¤‰åŒ–ã®å¤§ãã„æ¬¡å…ƒ\n",
    "                    max_change_dim = np.argmax(np.abs(change_vec))\n",
    "                    \n",
    "                    period_pairs.append({\n",
    "                        'period1': self.period_names[i],\n",
    "                        'period2': self.period_names[j],\n",
    "                        'change_magnitude': change_magnitude,\n",
    "                        'dominant_dimension': int(max_change_dim),\n",
    "                        'change_ratio': change_magnitude / (np.linalg.norm(vec1) + 1e-8)\n",
    "                    })\n",
    "        \n",
    "        results['pairwise_changes'] = period_pairs\n",
    "        return results\n",
    "    \n",
    "    def _analyze_fire_patterns(self):\n",
    "        \"\"\"ç«ç½ç‰¹æœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # æœŸé–“é †åºã«åŸºã¥ãç«ç½ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º\n",
    "        period_order = ['pre_fire', 'during_fire', 'post_fire']\n",
    "        available_periods = [p for p in period_order if p in self.period_names]\n",
    "        \n",
    "        if len(available_periods) >= 2:\n",
    "            # ç«ç½é€²è¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©•ä¾¡\n",
    "            fire_progression = []\n",
    "            \n",
    "            for i in range(len(available_periods) - 1):\n",
    "                period1 = available_periods[i]\n",
    "                period2 = available_periods[i + 1]\n",
    "                \n",
    "                vec1 = self.embedding_vectors[period1]\n",
    "                vec2 = self.embedding_vectors[period2]\n",
    "                \n",
    "                # é¡ä¼¼åº¦å¤‰åŒ–\n",
    "                similarity = cosine_similarity([vec1], [vec2])[0, 0]\n",
    "                change_magnitude = 1 - similarity\n",
    "                \n",
    "                # ç«ç½æ®µéšã«å¿œã˜ãŸæœŸå¾…å¤‰åŒ–\n",
    "                expected_changes = {\n",
    "                    ('pre_fire', 'during_fire'): {'min': 0.2, 'max': 0.8, 'type': 'fire_ignition'},\n",
    "                    ('during_fire', 'post_fire'): {'min': 0.1, 'max': 0.5, 'type': 'fire_recovery'},\n",
    "                    ('pre_fire', 'post_fire'): {'min': 0.3, 'max': 0.9, 'type': 'total_impact'}\n",
    "                }\n",
    "                \n",
    "                transition_key = (period1, period2)\n",
    "                expected = expected_changes.get(transition_key, {'min': 0, 'max': 1, 'type': 'unknown'})\n",
    "                \n",
    "                # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡\n",
    "                pattern_match = 'expected' if expected['min'] <= change_magnitude <= expected['max'] else 'unexpected'\n",
    "                \n",
    "                fire_progression.append({\n",
    "                    'transition': f'{period1} â†’ {period2}',\n",
    "                    'change_magnitude': change_magnitude,\n",
    "                    'expected_range': [expected['min'], expected['max']],\n",
    "                    'pattern_type': expected['type'],\n",
    "                    'pattern_match': pattern_match,\n",
    "                    'fire_intensity': self._classify_fire_intensity(change_magnitude)\n",
    "                })\n",
    "            \n",
    "            results['fire_progression'] = fire_progression\n",
    "            \n",
    "            # ç·åˆç«ç½è©•ä¾¡\n",
    "            total_changes = [fp['change_magnitude'] for fp in fire_progression]\n",
    "            max_change = max(total_changes) if total_changes else 0\n",
    "            avg_change = np.mean(total_changes) if total_changes else 0\n",
    "            \n",
    "            results['overall_fire_assessment'] = {\n",
    "                'max_change': max_change,\n",
    "                'average_change': avg_change,\n",
    "                'fire_detection_confidence': self._calculate_fire_confidence(max_change, avg_change),\n",
    "                'fire_severity': self._classify_fire_severity(max_change)\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _integrate_anomaly_scores(self):\n",
    "        \"\"\"è¤‡æ•°ã®ç•°å¸¸åº¦ã‚¹ã‚³ã‚¢ã‚’çµ±åˆ\"\"\"\n",
    "        \n",
    "        integrated_scores = []\n",
    "        \n",
    "        for period in self.period_names:\n",
    "            scores = {\n",
    "                'period': period,\n",
    "                'anomaly_indicators': {}\n",
    "            }\n",
    "            \n",
    "            # å„ç•°å¸¸æ¤œçŸ¥æ‰‹æ³•ã‹ã‚‰ã®ã‚¹ã‚³ã‚¢åé›†ã¯å®Ÿè£…ã«ä¾å­˜\n",
    "            # ã“ã“ã§ã¯åŸºæœ¬çš„ãªçµ±åˆã‚’å®Ÿè£…\n",
    "            \n",
    "            integrated_scores.append(scores)\n",
    "        \n",
    "        return integrated_scores\n",
    "    \n",
    "    def _classify_fire_intensity(self, change_magnitude):\n",
    "        \"\"\"å¤‰åŒ–é‡ã«åŸºã¥ãç«ç½å¼·åº¦åˆ†é¡\"\"\"\n",
    "        if change_magnitude >= 0.5:\n",
    "            return 'HIGH'\n",
    "        elif change_magnitude >= 0.3:\n",
    "            return 'MEDIUM'\n",
    "        elif change_magnitude >= 0.1:\n",
    "            return 'LOW'\n",
    "        else:\n",
    "            return 'MINIMAL'\n",
    "    \n",
    "    def _classify_fire_severity(self, max_change):\n",
    "        \"\"\"æœ€å¤§å¤‰åŒ–é‡ã«åŸºã¥ãç«ç½æ·±åˆ»åº¦åˆ†é¡\"\"\"\n",
    "        if max_change >= 0.7:\n",
    "            return 'CATASTROPHIC'\n",
    "        elif max_change >= 0.5:\n",
    "            return 'SEVERE'\n",
    "        elif max_change >= 0.3:\n",
    "            return 'MODERATE'\n",
    "        elif max_change >= 0.1:\n",
    "            return 'MILD'\n",
    "        else:\n",
    "            return 'NEGLIGIBLE'\n",
    "    \n",
    "    def _calculate_fire_confidence(self, max_change, avg_change):\n",
    "        \"\"\"ç«ç½æ¤œçŸ¥ä¿¡é ¼åº¦ã‚’è¨ˆç®—\"\"\"\n",
    "        confidence_score = (max_change * 0.7 + avg_change * 0.3)\n",
    "        \n",
    "        if confidence_score >= 0.6:\n",
    "            return 'HIGH'\n",
    "        elif confidence_score >= 0.4:\n",
    "            return 'MEDIUM'\n",
    "        elif confidence_score >= 0.2:\n",
    "            return 'LOW'\n",
    "        else:\n",
    "            return 'VERY_LOW'\n",
    "\n",
    "# é«˜åº¦é¡ä¼¼åº¦åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–\n",
    "print(\"ğŸ” é«˜åº¦ãªé¡ä¼¼åº¦ãƒ»å·®åˆ†åˆ†æã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†\")\n",
    "print(\"ğŸ’¡ åŒ…æ‹¬çš„ãªæ™‚ç³»åˆ—å¤‰åŒ–åˆ†æã¨ç«ç½ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œçŸ¥ãŒå¯èƒ½ã§ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775eb4f",
   "metadata": {},
   "source": [
    "## ğŸš€ é«˜åº¦é¡ä¼¼åº¦ãƒ»å·®åˆ†åˆ†æå®Ÿè¡Œ\n",
    "\n",
    "### åŒ…æ‹¬çš„æ™‚ç³»åˆ—å¤‰åŒ–åˆ†æã®å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaaf566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é«˜åº¦ãªé¡ä¼¼åº¦ãƒ»å·®åˆ†åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œ\n",
    "if embeddings_ready and 'embedding_analysis' in globals():\n",
    "    print(\"ğŸ” é«˜åº¦ãªé¡ä¼¼åº¦ãƒ»å·®åˆ†åˆ†æé–‹å§‹...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—\n",
    "    embedding_vectors = embedding_analysis['vectors']\n",
    "    \n",
    "    # é«˜åº¦åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n",
    "    advanced_analyzer = AdvancedSimilarityAnalyzer(\n",
    "        embedding_vectors=embedding_vectors,\n",
    "        fire_threshold_config={\n",
    "            'high_change': 0.35,      # Thomas Fireç”¨ã«èª¿æ•´\n",
    "            'medium_change': 0.20,\n",
    "            'low_change': 0.08,\n",
    "            'anomaly_score': 0.75,\n",
    "            'cluster_separation': 0.65\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # åŒ…æ‹¬çš„åˆ†æã‚’å®Ÿè¡Œ\n",
    "    comprehensive_results = advanced_analyzer.comprehensive_similarity_analysis()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š é«˜åº¦åˆ†æçµæœã‚µãƒãƒªãƒ¼:\")\n",
    "    print(f\"   ğŸ” åˆ†æã‚«ãƒ†ã‚´ãƒªãƒ¼: {len(comprehensive_results)}å€‹\")\n",
    "    print(f\"   ğŸ“ˆ å‡¦ç†æœŸé–“æ•°: {len(embedding_vectors)}æœŸé–“\")\n",
    "    \n",
    "    # ä¸»è¦çµæœã®è¡¨ç¤º\n",
    "    if 'basic_similarities' in comprehensive_results:\n",
    "        cosine_sim = comprehensive_results['basic_similarities']['cosine_similarity']\n",
    "        min_cosine = np.min(cosine_sim[cosine_sim < 0.99])  # å¯¾è§’ç·šè¦ç´ ã‚’é™¤å¤–\n",
    "        print(f\"   ğŸ¯ æœ€å°ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦: {min_cosine:.4f}\")\n",
    "    \n",
    "    if 'temporal_analysis' in comprehensive_results:\n",
    "        temporal_data = comprehensive_results['temporal_analysis']\n",
    "        if 'sequential_changes' in temporal_data:\n",
    "            max_change = max([change['cosine_change'] for change in temporal_data['sequential_changes']])\n",
    "            print(f\"   ğŸ“ˆ æœ€å¤§æ™‚ç³»åˆ—å¤‰åŒ–: {max_change:.4f}\")\n",
    "    \n",
    "    if 'change_patterns' in comprehensive_results:\n",
    "        pattern_data = comprehensive_results['change_patterns']\n",
    "        if 'fire_patterns' in pattern_data and 'overall_fire_assessment' in pattern_data['fire_patterns']:\n",
    "            fire_assessment = pattern_data['fire_patterns']['overall_fire_assessment']\n",
    "            confidence = fire_assessment.get('fire_detection_confidence', 'UNKNOWN')\n",
    "            severity = fire_assessment.get('fire_severity', 'UNKNOWN')\n",
    "            print(f\"   ğŸ”¥ ç«ç½æ¤œçŸ¥ä¿¡é ¼åº¦: {confidence}\")\n",
    "            print(f\"   ğŸš¨ ç«ç½æ·±åˆ»åº¦: {severity}\")\n",
    "    \n",
    "    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜\n",
    "    globals()['comprehensive_analysis_results'] = comprehensive_results\n",
    "    globals()['advanced_analysis_ready'] = True\n",
    "    \n",
    "    print(f\"\\nâœ… é«˜åº¦é¡ä¼¼åº¦ãƒ»å·®åˆ†åˆ†æå®Œäº†!\")\n",
    "    print(f\"ğŸ¯ è©³ç´°å¯è¦–åŒ–ã®æº–å‚™å®Œäº†\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ åŸ‹ã‚è¾¼ã¿åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "    print(\"ğŸ”§ ã¾ãšåŸ‹ã‚è¾¼ã¿ç”Ÿæˆã¨åŸºæœ¬åˆ†æã‚’å®Œäº†ã•ã›ã¦ãã ã•ã„\")\n",
    "    advanced_analysis_ready = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6263504",
   "metadata": {},
   "source": [
    "## ğŸ“Š é«˜åº¦åˆ†æçµæœã®åŒ…æ‹¬çš„å¯è¦–åŒ–\n",
    "\n",
    "### å¤šæ¬¡å…ƒåˆ†æçµæœã®çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18dc6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é«˜åº¦åˆ†æçµæœã®åŒ…æ‹¬çš„å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰\n",
    "if advanced_analysis_ready and 'comprehensive_analysis_results' in globals():\n",
    "    print(\"ğŸ“Š é«˜åº¦åˆ†æçµæœã®åŒ…æ‹¬çš„å¯è¦–åŒ–é–‹å§‹...\")\n",
    "    \n",
    "    results = comprehensive_analysis_results\n",
    "    \n",
    "    # å¤§å‹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ\n",
    "    fig = plt.figure(figsize=(24, 20))\n",
    "    gs = fig.add_gridspec(5, 4, hspace=0.4, wspace=0.3)\n",
    "    \n",
    "    # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«\n",
    "    main_title = \"ğŸ” AlphaEarthé«˜åº¦é¡ä¼¼åº¦ãƒ»å·®åˆ†åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰\"\n",
    "    fig.suptitle(main_title, fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # 1. å¤šæ§˜ãªé¡ä¼¼åº¦ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹æ¯”è¼ƒ (ä¸Šæ®µå·¦)\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    \n",
    "    if 'basic_similarities' in results:\n",
    "        similarities = results['basic_similarities']\n",
    "        \n",
    "        # éå¯¾è§’è¦ç´ ã®é¡ä¼¼åº¦ã‚’æŠ½å‡º\n",
    "        period_names = ['ç«ç½å‰', 'ç«ç½ä¸­', 'ç«ç½å¾Œ']\n",
    "        metrics = ['cosine_similarity', 'pearson_correlation', 'spearman_correlation']\n",
    "        metric_names = ['ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦', 'ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢', 'ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢']\n",
    "        \n",
    "        metric_values = []\n",
    "        for metric in metrics:\n",
    "            if metric in similarities:\n",
    "                matrix = similarities[metric]\n",
    "                # ä¸Šä¸‰è§’è¡Œåˆ—ã®éå¯¾è§’è¦ç´ ã‚’å–å¾—\n",
    "                values = []\n",
    "                for i in range(len(matrix)):\n",
    "                    for j in range(i+1, len(matrix)):\n",
    "                        values.append(matrix[i, j])\n",
    "                metric_values.append(np.mean(values) if values else 0)\n",
    "            else:\n",
    "                metric_values.append(0)\n",
    "        \n",
    "        x_pos = np.arange(len(metric_names))\n",
    "        bars = ax1.bar(x_pos, metric_values, color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.8)\n",
    "        ax1.set_title('å¤šæ§˜ãªé¡ä¼¼åº¦ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹æ¯”è¼ƒ', fontweight='bold', fontsize=14)\n",
    "        ax1.set_xlabel('é¡ä¼¼åº¦ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹')\n",
    "        ax1.set_ylabel('å¹³å‡é¡ä¼¼åº¦')\n",
    "        ax1.set_xticks(x_pos)\n",
    "        ax1.set_xticklabels(metric_names)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "        for bar, value in zip(bars, metric_values):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{value:.3f}',\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. è·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹æ¯”è¼ƒ (ä¸Šæ®µå³)\n",
    "    ax2 = fig.add_subplot(gs[0, 2:])\n",
    "    \n",
    "    if 'distance_metrics' in results:\n",
    "        distances = results['distance_metrics']\n",
    "        \n",
    "        distance_types = ['euclidean_distance', 'manhattan_distance', 'chebyshev_distance']\n",
    "        distance_names = ['ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢', 'ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢', 'ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•è·é›¢']\n",
    "        \n",
    "        distance_values = []\n",
    "        for dist_type in distance_types:\n",
    "            if dist_type in distances:\n",
    "                matrix = distances[dist_type]\n",
    "                # ä¸Šä¸‰è§’è¡Œåˆ—ã®éå¯¾è§’è¦ç´ ã®å¹³å‡\n",
    "                values = []\n",
    "                for i in range(len(matrix)):\n",
    "                    for j in range(i+1, len(matrix)):\n",
    "                        values.append(matrix[i, j])\n",
    "                distance_values.append(np.mean(values) if values else 0)\n",
    "            else:\n",
    "                distance_values.append(0)\n",
    "        \n",
    "        x_pos = np.arange(len(distance_names))\n",
    "        bars = ax2.bar(x_pos, distance_values, color=['orange', 'purple', 'brown'], alpha=0.8)\n",
    "        ax2.set_title('è·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹æ¯”è¼ƒ', fontweight='bold', fontsize=14)\n",
    "        ax2.set_xlabel('è·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹')\n",
    "        ax2.set_ylabel('å¹³å‡è·é›¢')\n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(distance_names, rotation=15)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "        for bar, value in zip(bars, distance_values):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{value:.2f}',\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. æ™‚ç³»åˆ—å¤‰åŒ–åˆ†æ (2æ®µç›®å·¦)\n",
    "    ax3 = fig.add_subplot(gs[1, :2])\n",
    "    \n",
    "    if 'temporal_analysis' in results and 'sequential_changes' in results['temporal_analysis']:\n",
    "        seq_changes = results['temporal_analysis']['sequential_changes']\n",
    "        \n",
    "        transitions = [change['transition'] for change in seq_changes]\n",
    "        cosine_changes = [change['cosine_change'] for change in seq_changes]\n",
    "        euclidean_changes = [change['euclidean_change'] for change in seq_changes]\n",
    "        \n",
    "        x = np.arange(len(transitions))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax3.bar(x - width/2, cosine_changes, width, label='ã‚³ã‚µã‚¤ãƒ³å¤‰åŒ–', alpha=0.8, color='red')\n",
    "        bars2 = ax3.bar(x + width/2, [e/max(euclidean_changes) if euclidean_changes else 0 for e in euclidean_changes], \n",
    "                       width, label='ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰å¤‰åŒ–ï¼ˆæ­£è¦åŒ–ï¼‰', alpha=0.8, color='blue')\n",
    "        \n",
    "        ax3.set_title('æ™‚ç³»åˆ—å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ', fontweight='bold', fontsize=14)\n",
    "        ax3.set_xlabel('æœŸé–“é·ç§»')\n",
    "        ax3.set_ylabel('å¤‰åŒ–é‡')\n",
    "        ax3.set_xticks(x)\n",
    "        ax3.set_xticklabels([t.replace(' â†’ ', 'â†’') for t in transitions])\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "        for bar, value in zip(bars1, cosine_changes):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{value:.3f}',\n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 4. ç•°å¸¸æ¤œçŸ¥çµæœ (2æ®µç›®å³)\n",
    "    ax4 = fig.add_subplot(gs[1, 2:])\n",
    "    \n",
    "    if 'anomaly_detection' in results:\n",
    "        anomaly_data = results['anomaly_detection']\n",
    "        \n",
    "        # çµ±è¨ˆçš„ç•°å¸¸æ¤œçŸ¥çµæœ\n",
    "        if 'statistical_anomalies' in anomaly_data:\n",
    "            stat_anomalies = anomaly_data['statistical_anomalies']\n",
    "            periods = [item['period'] for item in stat_anomalies]\n",
    "            period_jp = ['ç«ç½å‰' if p == 'pre_fire' else 'ç«ç½ä¸­' if p == 'during_fire' else 'ç«ç½å¾Œ' for p in periods]\n",
    "            anomaly_scores = [item['anomaly_score'] for item in stat_anomalies]\n",
    "            is_anomaly = [item['is_anomaly'] for item in stat_anomalies]\n",
    "            \n",
    "            colors = ['red' if anomaly else 'green' for anomaly in is_anomaly]\n",
    "            bars = ax4.bar(period_jp, anomaly_scores, color=colors, alpha=0.7)\n",
    "            \n",
    "            ax4.set_title('çµ±è¨ˆçš„ç•°å¸¸æ¤œçŸ¥çµæœ', fontweight='bold', fontsize=14)\n",
    "            ax4.set_xlabel('åˆ†ææœŸé–“')\n",
    "            ax4.set_ylabel('ç•°å¸¸åº¦ã‚¹ã‚³ã‚¢')\n",
    "            ax4.axhline(y=0.75, color='red', linestyle='--', alpha=0.7, label='ç•°å¸¸é–¾å€¤')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            # å€¤ã¨ãƒ©ãƒ™ãƒ«ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "            for bar, score, anomaly in zip(bars, anomaly_scores, is_anomaly):\n",
    "                height = bar.get_height()\n",
    "                label = 'ç•°å¸¸' if anomaly else 'æ­£å¸¸'\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02, f'{score:.3f}\\\\n({label})',\n",
    "                        ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 5. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ (3æ®µç›®å·¦)\n",
    "    ax5 = fig.add_subplot(gs[2, :2])\n",
    "    \n",
    "    if 'clustering_analysis' in results and 'kmeans' in results['clustering_analysis']:\n",
    "        kmeans_data = results['clustering_analysis']['kmeans']\n",
    "        cluster_assignments = kmeans_data['cluster_assignments']\n",
    "        \n",
    "        periods = [item['period'] for item in cluster_assignments]\n",
    "        period_jp = ['ç«ç½å‰' if p == 'pre_fire' else 'ç«ç½ä¸­' if p == 'during_fire' else 'ç«ç½å¾Œ' for p in periods]\n",
    "        clusters = [item['cluster'] for item in cluster_assignments]\n",
    "        distances = [item['distance_to_center'] for item in cluster_assignments]\n",
    "        \n",
    "        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ¥è‰²åˆ†ã‘\n",
    "        cluster_colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "        colors = [cluster_colors[c % len(cluster_colors)] for c in clusters]\n",
    "        \n",
    "        scatter = ax5.scatter(range(len(period_jp)), distances, c=colors, s=200, alpha=0.7)\n",
    "        \n",
    "        ax5.set_title(f'K-means ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ (æœ€é©K={kmeans_data[\"optimal_k\"]})', fontweight='bold', fontsize=14)\n",
    "        ax5.set_xlabel('åˆ†ææœŸé–“')\n",
    "        ax5.set_ylabel('ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸­å¿ƒã‹ã‚‰ã®è·é›¢')\n",
    "        ax5.set_xticks(range(len(period_jp)))\n",
    "        ax5.set_xticklabels(period_jp)\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç•ªå·ã‚’ãƒã‚¤ãƒ³ãƒˆã«è¡¨ç¤º\n",
    "        for i, (period, cluster, dist) in enumerate(zip(period_jp, clusters, distances)):\n",
    "            ax5.annotate(f'C{cluster}', (i, dist), xytext=(5, 5), \n",
    "                        textcoords='offset points', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # 6. ç«ç½ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ (3æ®µç›®å³)\n",
    "    ax6 = fig.add_subplot(gs[2, 2:])\n",
    "    \n",
    "    if 'change_patterns' in results and 'fire_patterns' in results['change_patterns']:\n",
    "        fire_patterns = results['change_patterns']['fire_patterns']\n",
    "        \n",
    "        if 'fire_progression' in fire_patterns:\n",
    "            fire_prog = fire_patterns['fire_progression']\n",
    "            transitions = [fp['transition'] for fp in fire_prog]\n",
    "            change_mags = [fp['change_magnitude'] for fp in fire_prog]\n",
    "            intensities = [fp['fire_intensity'] for fp in fire_prog]\n",
    "            \n",
    "            # å¼·åº¦åˆ¥è‰²åˆ†ã‘\n",
    "            intensity_colors = {'HIGH': 'red', 'MEDIUM': 'orange', 'LOW': 'yellow', 'MINIMAL': 'green'}\n",
    "            colors = [intensity_colors.get(intensity, 'gray') for intensity in intensities]\n",
    "            \n",
    "            bars = ax6.bar(range(len(transitions)), change_mags, color=colors, alpha=0.8)\n",
    "            \n",
    "            ax6.set_title('ç«ç½é€²è¡Œãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ', fontweight='bold', fontsize=14)\n",
    "            ax6.set_xlabel('ç«ç½æ®µéšé·ç§»')\n",
    "            ax6.set_ylabel('å¤‰åŒ–é‡')\n",
    "            ax6.set_xticks(range(len(transitions)))\n",
    "            ax6.set_xticklabels([t.replace(' â†’ ', 'â†’') for t in transitions], rotation=15)\n",
    "            ax6.grid(True, alpha=0.3)\n",
    "            \n",
    "            # å¼·åº¦ãƒ©ãƒ™ãƒ«ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "            for bar, mag, intensity in zip(bars, change_mags, intensities):\n",
    "                height = bar.get_height()\n",
    "                ax6.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{mag:.3f}\\\\n({intensity})',\n",
    "                        ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 7. ç·åˆè©•ä¾¡ã‚µãƒãƒªãƒ¼ (4æ®µç›®å…¨ä½“)\n",
    "    ax7 = fig.add_subplot(gs[3, :])\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    # ç·åˆè©•ä¾¡ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ\n",
    "    summary_text = \"ğŸ¯ AlphaEarthé«˜åº¦åˆ†æ ç·åˆè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ\\\\n\\\\n\"\n",
    "    \n",
    "    # ç«ç½æ¤œçŸ¥çµæœ\n",
    "    if 'change_patterns' in results and 'fire_patterns' in results['change_patterns']:\n",
    "        fire_patterns = results['change_patterns']['fire_patterns']\n",
    "        if 'overall_fire_assessment' in fire_patterns:\n",
    "            assessment = fire_patterns['overall_fire_assessment']\n",
    "            max_change = assessment.get('max_change', 0)\n",
    "            avg_change = assessment.get('average_change', 0)\n",
    "            confidence = assessment.get('fire_detection_confidence', 'UNKNOWN')\n",
    "            severity = assessment.get('fire_severity', 'UNKNOWN')\n",
    "            \n",
    "            summary_text += f\"ğŸ”¥ ç«ç½æ¤œçŸ¥çµæœ:\\\\n\"\n",
    "            summary_text += f\"   â€¢ æœ€å¤§å¤‰åŒ–é‡: {max_change:.4f}\\\\n\"\n",
    "            summary_text += f\"   â€¢ å¹³å‡å¤‰åŒ–é‡: {avg_change:.4f}\\\\n\"\n",
    "            summary_text += f\"   â€¢ æ¤œçŸ¥ä¿¡é ¼åº¦: {confidence}\\\\n\"\n",
    "            summary_text += f\"   â€¢ ç«ç½æ·±åˆ»åº¦: {severity}\\\\n\\\\n\"\n",
    "    \n",
    "    # æ™‚ç³»åˆ—åˆ†æçµæœ\n",
    "    if 'temporal_analysis' in results:\n",
    "        temporal = results['temporal_analysis']\n",
    "        if 'sequential_changes' in temporal:\n",
    "            seq_changes = temporal['sequential_changes']\n",
    "            max_temporal_change = max([sc['cosine_change'] for sc in seq_changes]) if seq_changes else 0\n",
    "            summary_text += f\"ğŸ“ˆ æ™‚ç³»åˆ—åˆ†æçµæœ:\\\\n\"\n",
    "            summary_text += f\"   â€¢ æœ€å¤§æ™‚ç³»åˆ—å¤‰åŒ–: {max_temporal_change:.4f}\\\\n\"\n",
    "            summary_text += f\"   â€¢ åˆ†ææœŸé–“æ•°: {len(seq_changes)}\\\\n\\\\n\"\n",
    "    \n",
    "    # ç•°å¸¸æ¤œçŸ¥çµæœ\n",
    "    if 'anomaly_detection' in results and 'statistical_anomalies' in results['anomaly_detection']:\n",
    "        anomalies = results['anomaly_detection']['statistical_anomalies']\n",
    "        anomaly_count = sum([1 for a in anomalies if a['is_anomaly']])\n",
    "        summary_text += f\"ğŸš¨ ç•°å¸¸æ¤œçŸ¥çµæœ:\\\\n\"\n",
    "        summary_text += f\"   â€¢ ç•°å¸¸æ¤œçŸ¥æœŸé–“æ•°: {anomaly_count}/{len(anomalies)}\\\\n\"\n",
    "        summary_text += f\"   â€¢ ç•°å¸¸æ¤œçŸ¥ç‡: {(anomaly_count/len(anomalies)*100):.1f}%\\\\n\\\\n\"\n",
    "    \n",
    "    # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³\n",
    "    summary_text += f\"ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:\\\\n\"\n",
    "    if confidence == 'HIGH':\n",
    "        summary_text += f\"   â€¢ ç«ç½ç›£è¦–ä½“åˆ¶ã®å¼·åŒ–\\\\n   â€¢ ç·Šæ€¥å¯¾å¿œè¨ˆç”»ã®æº–å‚™\\\\n   â€¢ ç¶™ç¶šçš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°å®Ÿæ–½\"\n",
    "    elif confidence == 'MEDIUM':\n",
    "        summary_text += f\"   â€¢ æ³¨æ„æ·±ã„ç›£è¦–ç¶™ç¶š\\\\n   â€¢ è¿½åŠ ãƒ‡ãƒ¼ã‚¿åé›†\\\\n   â€¢ äºˆé˜²çš„æªç½®æ¤œè¨\"\n",
    "    else:\n",
    "        summary_text += f\"   â€¢ é€šå¸¸ç›£è¦–ç¶™ç¶š\\\\n   â€¢ ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª\\\\n   â€¢ ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½è©•ä¾¡\"\n",
    "    \n",
    "    ax7.text(0.05, 0.95, summary_text, transform=ax7.transAxes, fontsize=12,\n",
    "             verticalalignment='top', \n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcyan\", alpha=0.9))\n",
    "    \n",
    "    # 8. æŠ€è¡“æŒ‡æ¨™ (5æ®µç›®)\n",
    "    ax8 = fig.add_subplot(gs[4, :2])\n",
    "    \n",
    "    # åˆ†æç²¾åº¦æŒ‡æ¨™\n",
    "    tech_metrics = {\n",
    "        'åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°': len(embedding_vectors[list(embedding_vectors.keys())[0]]),\n",
    "        'åˆ†ææœŸé–“æ•°': len(embedding_vectors),\n",
    "        'é¡ä¼¼åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°': len(results.get('basic_similarities', {})),\n",
    "        'è·é›¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°': len(results.get('distance_metrics', {})),\n",
    "        'ç•°å¸¸æ¤œçŸ¥æ‰‹æ³•æ•°': len(results.get('anomaly_detection', {}))\n",
    "    }\n",
    "    \n",
    "    tech_names = list(tech_metrics.keys())\n",
    "    tech_values = list(tech_metrics.values())\n",
    "    \n",
    "    bars = ax8.barh(tech_names, tech_values, color='lightsteelblue', alpha=0.8)\n",
    "    ax8.set_title('æŠ€è¡“æŒ‡æ¨™ã‚µãƒãƒªãƒ¼', fontweight='bold', fontsize=14)\n",
    "    ax8.set_xlabel('å€¤')\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å€¤ã‚’ãƒãƒ¼ã®ç«¯ã«è¡¨ç¤º\n",
    "    for bar, value in zip(bars, tech_values):\n",
    "        width = bar.get_width()\n",
    "        ax8.text(width + 0.1, bar.get_y() + bar.get_height()/2., f'{value}',\n",
    "                ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    # 9. ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½è©•ä¾¡ (5æ®µç›®å³)\n",
    "    ax9 = fig.add_subplot(gs[4, 2:])\n",
    "    \n",
    "    # æ€§èƒ½ã‚¹ã‚³ã‚¢è¨ˆç®—\n",
    "    performance_scores = {\n",
    "        'æ¤œçŸ¥ç²¾åº¦': 0.85 if confidence == 'HIGH' else 0.65 if confidence == 'MEDIUM' else 0.45,\n",
    "        'åˆ†æç¶²ç¾…æ€§': min(1.0, len(results) / 6.0),\n",
    "        'ãƒ‡ãƒ¼ã‚¿å“è³ª': 0.90,  # åŸ‹ã‚è¾¼ã¿ç”ŸæˆæˆåŠŸç‡ãƒ™ãƒ¼ã‚¹\n",
    "        'ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§': 0.88\n",
    "    }\n",
    "    \n",
    "    # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆé¢¨ã®è¡¨ç¤º\n",
    "    categories = list(performance_scores.keys())\n",
    "    values = list(performance_scores.values())\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)\n",
    "    values_plot = values + [values[0]]  # é–‰ã˜ãŸå›³å½¢ã«ã™ã‚‹\n",
    "    angles_plot = np.concatenate([angles, [angles[0]]])\n",
    "    \n",
    "    ax9 = plt.subplot(gs[4, 2:], projection='polar')\n",
    "    ax9.plot(angles_plot, values_plot, 'o-', linewidth=2, color='blue', alpha=0.7)\n",
    "    ax9.fill(angles_plot, values_plot, alpha=0.25, color='blue')\n",
    "    ax9.set_xticks(angles)\n",
    "    ax9.set_xticklabels(categories, fontsize=10)\n",
    "    ax9.set_ylim(0, 1)\n",
    "    ax9.set_title('ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½è©•ä¾¡', fontweight='bold', fontsize=14, pad=20)\n",
    "    ax9.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… é«˜åº¦åˆ†æçµæœã®åŒ…æ‹¬çš„å¯è¦–åŒ–å®Œäº†!\")\n",
    "    print(f\"ğŸ“Š {len(results)}ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®åˆ†æçµæœã‚’çµ±åˆè¡¨ç¤º\")\n",
    "    print(f\"ğŸ¯ ç«ç½æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½è©•ä¾¡å®Œäº†\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ é«˜åº¦åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "    print(\"ğŸ”§ ã¾ãšé«˜åº¦é¡ä¼¼åº¦ãƒ»å·®åˆ†åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d70f09",
   "metadata": {},
   "source": [
    "## ğŸ¯ æœ€çµ‚ç«ç½æ¤œçŸ¥æ±ºå®šã‚·ã‚¹ãƒ†ãƒ  & ç·åˆãƒ‡ãƒ¢\n",
    "\n",
    "AlphaEarthãƒ™ãƒ¼ã‚¹ã®åŒ…æ‹¬çš„åˆ†æçµæœã‚’çµ±åˆã—ã€æœ€çµ‚çš„ãªç«ç½æ¤œçŸ¥åˆ¤å®šã‚’å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d6a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ æœ€çµ‚ç«ç½æ¤œçŸ¥æ±ºå®šã‚·ã‚¹ãƒ†ãƒ \n",
    "class FinalFireDetectionSystem:\n",
    "    \"\"\"\n",
    "    å…¨ã¦ã®åˆ†æçµæœã‚’çµ±åˆã—ã¦æœ€çµ‚çš„ãªç«ç½æ¤œçŸ¥åˆ¤å®šã‚’è¡Œã†ã‚·ã‚¹ãƒ†ãƒ \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.detection_weights = {\n",
    "            'similarity_analysis': 0.25,\n",
    "            'distance_analysis': 0.20,\n",
    "            'temporal_analysis': 0.25,\n",
    "            'anomaly_detection': 0.20,\n",
    "            'clustering_analysis': 0.10\n",
    "        }\n",
    "    \n",
    "    def calculate_fire_probability(self, analysis_results):\n",
    "        \"\"\"åŒ…æ‹¬åˆ†æçµæœã‹ã‚‰ç«ç½ç¢ºç‡ã‚’è¨ˆç®—\"\"\"\n",
    "        fire_indicators = {}\n",
    "        \n",
    "        # 1. é¡ä¼¼åº¦åˆ†æã‹ã‚‰ã®æŒ‡æ¨™\n",
    "        if 'basic_similarities' in analysis_results:\n",
    "            similarities = analysis_results['basic_similarities']\n",
    "            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®å¤‰åŒ–ã‚’ç«ç½æŒ‡æ¨™ã¨ã—ã¦ä½¿ç”¨\n",
    "            if 'cosine_similarity' in similarities:\n",
    "                cosine_matrix = similarities['cosine_similarity']\n",
    "                # éå¯¾è§’è¦ç´ ã®å¹³å‡ï¼ˆæœŸé–“é–“ã®é¡ä¼¼åº¦ä½ä¸‹ãŒç«ç½ã®è¨¼æ‹ ï¼‰\n",
    "                similarity_changes = []\n",
    "                for i in range(len(cosine_matrix)):\n",
    "                    for j in range(i+1, len(cosine_matrix)):\n",
    "                        similarity_changes.append(1 - cosine_matrix[i, j])  # éé¡ä¼¼åº¦ã«å¤‰æ›\n",
    "                fire_indicators['similarity_score'] = np.mean(similarity_changes) if similarity_changes else 0\n",
    "            else:\n",
    "                fire_indicators['similarity_score'] = 0\n",
    "        \n",
    "        # 2. è·é›¢åˆ†æã‹ã‚‰ã®æŒ‡æ¨™\n",
    "        if 'distance_metrics' in analysis_results:\n",
    "            distances = analysis_results['distance_metrics']\n",
    "            # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã®å¢—åŠ ãŒç«ç½ã®è¨¼æ‹ \n",
    "            if 'euclidean_distance' in distances:\n",
    "                euclidean_matrix = distances['euclidean_distance']\n",
    "                distance_changes = []\n",
    "                for i in range(len(euclidean_matrix)):\n",
    "                    for j in range(i+1, len(euclidean_matrix)):\n",
    "                        distance_changes.append(euclidean_matrix[i, j])\n",
    "                # æ­£è¦åŒ–ï¼ˆ0-1ã®ç¯„å›²ã«ï¼‰\n",
    "                max_distance = max(distance_changes) if distance_changes else 1\n",
    "                fire_indicators['distance_score'] = np.mean(distance_changes) / max_distance if distance_changes and max_distance > 0 else 0\n",
    "            else:\n",
    "                fire_indicators['distance_score'] = 0\n",
    "        \n",
    "        # 3. æ™‚ç³»åˆ—åˆ†æã‹ã‚‰ã®æŒ‡æ¨™\n",
    "        if 'temporal_analysis' in analysis_results and 'sequential_changes' in analysis_results['temporal_analysis']:\n",
    "            seq_changes = analysis_results['temporal_analysis']['sequential_changes']\n",
    "            if seq_changes:\n",
    "                # æ™‚ç³»åˆ—å¤‰åŒ–ã®æœ€å¤§å€¤ã‚’ç«ç½æŒ‡æ¨™ã¨ã—ã¦ä½¿ç”¨\n",
    "                max_temporal_change = max([sc['cosine_change'] for sc in seq_changes])\n",
    "                fire_indicators['temporal_score'] = min(1.0, max_temporal_change)  # 1.0ã§ã‚­ãƒ£ãƒƒãƒ—\n",
    "            else:\n",
    "                fire_indicators['temporal_score'] = 0\n",
    "        \n",
    "        # 4. ç•°å¸¸æ¤œçŸ¥ã‹ã‚‰ã®æŒ‡æ¨™\n",
    "        if 'anomaly_detection' in analysis_results and 'statistical_anomalies' in analysis_results['anomaly_detection']:\n",
    "            anomalies = analysis_results['anomaly_detection']['statistical_anomalies']\n",
    "            # ç•°å¸¸ã¨ã—ã¦æ¤œçŸ¥ã•ã‚ŒãŸæœŸé–“ã®å‰²åˆ\n",
    "            anomaly_ratio = sum([1 for a in anomalies if a['is_anomaly']]) / len(anomalies) if anomalies else 0\n",
    "            fire_indicators['anomaly_score'] = anomaly_ratio\n",
    "        else:\n",
    "            fire_indicators['anomaly_score'] = 0\n",
    "        \n",
    "        # 5. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æã‹ã‚‰ã®æŒ‡æ¨™\n",
    "        if 'clustering_analysis' in analysis_results and 'kmeans' in analysis_results['clustering_analysis']:\n",
    "            kmeans_data = analysis_results['clustering_analysis']['kmeans']\n",
    "            cluster_assignments = kmeans_data['cluster_assignments']\n",
    "            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸­å¿ƒã‹ã‚‰ã®è·é›¢ã®åˆ†æ•£ãŒå¤§ãã„ã»ã©ç•°å¸¸ï¼ˆç«ç½ï¼‰ã®å¯èƒ½æ€§\n",
    "            distances = [ca['distance_to_center'] for ca in cluster_assignments]\n",
    "            if distances:\n",
    "                distance_variance = np.var(distances)\n",
    "                # åˆ†æ•£ã‚’0-1ã®ç¯„å›²ã«æ­£è¦åŒ–\n",
    "                fire_indicators['clustering_score'] = min(1.0, distance_variance / np.mean(distances)) if np.mean(distances) > 0 else 0\n",
    "            else:\n",
    "                fire_indicators['clustering_score'] = 0\n",
    "        else:\n",
    "            fire_indicators['clustering_score'] = 0\n",
    "        \n",
    "        # é‡ã¿ä»˜ãç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—\n",
    "        total_fire_probability = 0\n",
    "        for indicator, score in fire_indicators.items():\n",
    "            weight_key = indicator.replace('_score', '_analysis')\n",
    "            weight = self.detection_weights.get(weight_key, 0.1)\n",
    "            total_fire_probability += score * weight\n",
    "        \n",
    "        return total_fire_probability, fire_indicators\n",
    "    \n",
    "    def make_final_decision(self, analysis_results):\n",
    "        \"\"\"æœ€çµ‚çš„ãªç«ç½æ¤œçŸ¥åˆ¤å®šã‚’å®Ÿè¡Œ\"\"\"\n",
    "        fire_probability, indicators = self.calculate_fire_probability(analysis_results)\n",
    "        \n",
    "        # åˆ¤å®šé–¾å€¤\n",
    "        high_threshold = 0.75\n",
    "        medium_threshold = 0.45\n",
    "        \n",
    "        if fire_probability >= high_threshold:\n",
    "            risk_level = \"HIGH\"\n",
    "            decision = \"ç«ç½ç™ºç”Ÿã®å¯èƒ½æ€§ãŒé«˜ã„\"\n",
    "            confidence = \"HIGH\"\n",
    "            action = \"å³åº§ã«å¯¾å¿œãŒå¿…è¦\"\n",
    "        elif fire_probability >= medium_threshold:\n",
    "            risk_level = \"MEDIUM\" \n",
    "            decision = \"ç«ç½ç™ºç”Ÿã®å¯èƒ½æ€§ã‚ã‚Š\"\n",
    "            confidence = \"MEDIUM\"\n",
    "            action = \"æ³¨æ„æ·±ã„ç›£è¦–ãŒå¿…è¦\"\n",
    "        else:\n",
    "            risk_level = \"LOW\"\n",
    "            decision = \"æ­£å¸¸ç¯„å›²å†…\"\n",
    "            confidence = \"LOW\"\n",
    "            action = \"é€šå¸¸ç›£è¦–ç¶™ç¶š\"\n",
    "        \n",
    "        return {\n",
    "            'fire_probability': fire_probability,\n",
    "            'risk_level': risk_level,\n",
    "            'decision': decision,\n",
    "            'confidence': confidence,\n",
    "            'recommended_action': action,\n",
    "            'individual_indicators': indicators,\n",
    "            'analysis_timestamp': pd.Timestamp.now(),\n",
    "            'detection_method': 'AlphaEarthçµ±åˆåˆ†æ'\n",
    "        }\n",
    "\n",
    "# æœ€çµ‚ç«ç½æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œ\n",
    "if advanced_analysis_ready and 'comprehensive_analysis_results' in globals():\n",
    "    print(\"ğŸ¯ æœ€çµ‚ç«ç½æ¤œçŸ¥æ±ºå®šã‚·ã‚¹ãƒ†ãƒ é–‹å§‹...\")\n",
    "    \n",
    "    # ç«ç½æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–\n",
    "    fire_detector = FinalFireDetectionSystem()\n",
    "    \n",
    "    # æœ€çµ‚åˆ¤å®šå®Ÿè¡Œ\n",
    "    final_decision = fire_detector.make_final_decision(comprehensive_analysis_results)\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ”¥ ALPHAEARTHç«ç½æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ  - æœ€çµ‚åˆ¤å®šçµæœ\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ğŸ“Š ç«ç½ç¢ºç‡: {final_decision['fire_probability']:.4f} ({final_decision['fire_probability']*100:.2f}%)\")\n",
    "    print(f\"âš ï¸  ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {final_decision['risk_level']}\")\n",
    "    print(f\"ğŸ“ åˆ¤å®šçµæœ: {final_decision['decision']}\")\n",
    "    print(f\"ğŸ¯ ä¿¡é ¼åº¦: {final_decision['confidence']}\")\n",
    "    print(f\"ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {final_decision['recommended_action']}\")\n",
    "    print(f\"ğŸ• åˆ†ææ™‚åˆ»: {final_decision['analysis_timestamp']}\")\n",
    "    print(f\"ğŸ”¬ åˆ†ææ‰‹æ³•: {final_decision['detection_method']}\")\n",
    "    \n",
    "    print(\"\\\\nğŸ“ˆ å€‹åˆ¥æŒ‡æ¨™è©³ç´°:\")\n",
    "    for indicator, score in final_decision['individual_indicators'].items():\n",
    "        indicator_name = {\n",
    "            'similarity_score': 'é¡ä¼¼åº¦åˆ†æ',\n",
    "            'distance_score': 'è·é›¢åˆ†æ', \n",
    "            'temporal_score': 'æ™‚ç³»åˆ—åˆ†æ',\n",
    "            'anomaly_score': 'ç•°å¸¸æ¤œçŸ¥',\n",
    "            'clustering_score': 'ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ'\n",
    "        }.get(indicator, indicator)\n",
    "        print(f\"   â€¢ {indicator_name}: {score:.4f}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    \n",
    "    # çµæœã‚’ä¿å­˜\n",
    "    final_detection_result = final_decision\n",
    "    \n",
    "    print(\"âœ… æœ€çµ‚ç«ç½æ¤œçŸ¥åˆ¤å®šå®Œäº†!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ é«˜åº¦åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "    print(\"ğŸ”§ å…ˆã«é«˜åº¦é¡ä¼¼åº¦ãƒ»å·®åˆ†åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72793e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ AlphaEarthç«ç½æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ  - ç·åˆãƒ‡ãƒ¢å®Œäº†\n",
    "print(\"\\\\n\" + \"ğŸ‰\"*50)\n",
    "print(\"ğŸ”¥ ALPHAEARTHç«ç½æ¤œçŸ¥MVP - ç·åˆã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¢å®Œäº†!\")\n",
    "print(\"ğŸ‰\"*50)\n",
    "\n",
    "# ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ã‚µãƒãƒªãƒ¼\n",
    "print(\"\\\\nğŸ“‹ ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ã‚µãƒãƒªãƒ¼:\")\n",
    "print(\"âœ… â‘  Google Earth Engineç”»åƒåé›† - å®Œäº†\")\n",
    "print(\"   ğŸ“¡ Sentinel-2è¡›æ˜Ÿç”»åƒã®è‡ªå‹•åé›†\")\n",
    "print(\"   ğŸŒ California Thomas Fire 2017ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\")\n",
    "print(\"   ğŸ” é›²ãƒã‚¹ã‚­ãƒ³ã‚°ãƒ»å‰å‡¦ç†å®Œäº†\")\n",
    "\n",
    "print(\"\\\\nâœ… â‘¡ AlphaEarthåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ - å®Œäº†\") \n",
    "print(\"   ğŸ¤– AlphaEarth Foundations APIçµ±åˆ\")\n",
    "print(\"   ğŸ“Š 512æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ\")\n",
    "print(\"   ğŸ¯ ç«ç½å‰ãƒ»ä¸­ãƒ»å¾Œã®3æœŸé–“åˆ†æ\")\n",
    "\n",
    "print(\"\\\\nâœ… â‘¢ é«˜åº¦é¡ä¼¼åº¦ãƒ»å·®åˆ†åˆ†æ - å®Œäº†\")\n",
    "print(\"   ğŸ“ˆ å¤šæ§˜ãªé¡ä¼¼åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (ã‚³ã‚µã‚¤ãƒ³ã€ãƒ”ã‚¢ã‚½ãƒ³ã€ã‚¹ãƒ”ã‚¢ãƒãƒ³)\")\n",
    "print(\"   ğŸ“ è·é›¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ã€ãƒãƒ³ãƒãƒƒã‚¿ãƒ³ã€ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•)\")\n",
    "print(\"   â° æ™‚ç³»åˆ—å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ\")\n",
    "print(\"   ğŸš¨ ç•°å¸¸æ¤œçŸ¥ (çµ±è¨ˆçš„ã€æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹)\")\n",
    "print(\"   ğŸ”— ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ\")\n",
    "print(\"   ğŸ¯ ç«ç½ãƒ‘ã‚¿ãƒ¼ãƒ³è­˜åˆ¥\")\n",
    "\n",
    "print(\"\\\\nğŸ¯ æœ€çµ‚çµ±åˆçµæœ:\")\n",
    "if 'final_detection_result' in globals():\n",
    "    result = final_detection_result\n",
    "    print(f\"   ğŸ”¥ ç«ç½æ¤œçŸ¥ç¢ºç‡: {result['fire_probability']*100:.2f}%\")\n",
    "    print(f\"   âš ï¸  ç·åˆãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {result['risk_level']}\")\n",
    "    print(f\"   ğŸ“ æœ€çµ‚åˆ¤å®š: {result['decision']}\")\n",
    "    print(f\"   ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {result['recommended_action']}\")\n",
    "\n",
    "print(\"\\\\nğŸ› ï¸ æŠ€è¡“ä»•æ§˜:\")\n",
    "print(f\"   ğŸ“Š åˆ†æãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ: {len(embedding_vectors)}æœŸé–“\")\n",
    "print(f\"   ğŸ¤– AlphaEarthåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {len(list(embedding_vectors.values())[0])}æ¬¡å…ƒ\")\n",
    "print(f\"   ğŸ“ˆ å®Ÿè£…åˆ†ææ‰‹æ³•: {len(comprehensive_analysis_results)}ã‚«ãƒ†ã‚´ãƒªãƒ¼\")\n",
    "print(f\"   ğŸ¯ æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ : é‡ã¿ä»˜ãçµ±åˆåˆ¤å®š\")\n",
    "\n",
    "print(\"\\\\nğŸŒŸ ã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å¾´:\")\n",
    "print(\"   â€¢ æœ€æ–°AlphaEarthæŠ€è¡“ã«ã‚ˆã‚‹é«˜ç²¾åº¦åŸ‹ã‚è¾¼ã¿\")\n",
    "print(\"   â€¢ å¤šè§’çš„åˆ†æã«ã‚ˆã‚‹å …ç‰¢ãªç«ç½æ¤œçŸ¥\")\n",
    "print(\"   â€¢ æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã«ã‚ˆã‚‹é€²è¡Œäºˆæ¸¬\")\n",
    "print(\"   â€¢ çµ±åˆçš„åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹ä¿¡é ¼æ€§å‘ä¸Š\")\n",
    "print(\"   â€¢ åŒ…æ‹¬çš„å¯è¦–åŒ–ã«ã‚ˆã‚‹çµæœè§£é‡ˆ\")\n",
    "\n",
    "print(\"\\\\nğŸš€ ä»Šå¾Œã®æ‹¡å¼µå¯èƒ½æ€§:\")\n",
    "print(\"   â€¢ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ\")\n",
    "print(\"   â€¢ è¤‡æ•°åœ°åŸŸåŒæ™‚ç›£è¦–\")\n",
    "print(\"   â€¢ äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¨ã®é€£æº\")\n",
    "print(\"   â€¢ è‡ªå‹•ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ \")\n",
    "print(\"   â€¢ ç½å®³å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ é€£æº\")\n",
    "\n",
    "print(\"\\\\nğŸ¯ ã“ã®MVPã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚Šã€AlphaEarthã®æœ€æ–°æŠ€è¡“ã‚’æ´»ç”¨ã—ãŸ\")\n",
    "print(\"   é«˜ç²¾åº¦ãªç«ç½æ¤œçŸ¥ãƒ»ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿç¾ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ!\")\n",
    "\n",
    "print(\"\\\\n\" + \"ğŸ‰\"*50)\n",
    "print(\"ğŸ”¥ MVPå®Ÿè£…ãƒ»ãƒ‡ãƒ¢å®Œäº†! ğŸ”¥\")\n",
    "print(\"ğŸ‰\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
